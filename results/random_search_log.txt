================================================================================
Trial 1 | time = 2025-11-15T23:25:03.744912
Command: python -m src.train_rl_ppo --config config_trial_001.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05  ->  0.05078
  - physchem_constraints.max_repeats: 2  ->  3
  - physchem_constraints.min_entropy: 3  ->  3.76
  - physchem_constraints.entropy_penalty_coef: 5.8  ->  7.435
  - reward_weights.physchem: 0.25  ->  0.4328
  - reward_weights.target_prob: 0.35  ->  0.468
  - active_selection.ucb.diversity_weight: 0.1  ->  0.4659
  - rl_enhanced.kl_penalty.beta: 0.02  ->  0.04325

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/32 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/32 [00:00<?, ?it/s, episode_len=19][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.13it/s, episode_len=19][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.13it/s, episode_len=3] [A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.13it/s, episode_len=4][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.13it/s, episode_len=12][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.13it/s, episode_len=2] [A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.13it/s, episode_len=5][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.13it/s, episode_len=20][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.13it/s, episode_len=2] [A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.13it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:00, 24.37it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:00, 24.37it/s, episode_len=11][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:00, 24.37it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:00, 24.37it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:00, 24.37it/s, episode_len=2] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 28.27it/s, episode_len=2][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 28.27it/s, episode_len=2][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 28.27it/s, episode_len=16][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 28.27it/s, episode_len=2] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 28.27it/s, episode_len=6][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 28.27it/s, episode_len=10][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 28.27it/s, episode_len=20][A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:00<00:00, 36.21it/s, episode_len=20][A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:00<00:00, 36.21it/s, episode_len=10][A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:00<00:00, 36.21it/s, episode_len=6] [A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:00<00:00, 36.21it/s, episode_len=20][A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:00<00:00, 36.21it/s, episode_len=4] [A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:00<00:00, 36.21it/s, episode_len=2][A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:00<00:00, 36.21it/s, episode_len=20][A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:00<00:00, 40.58it/s, episode_len=20][A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:00<00:00, 40.58it/s, episode_len=1] [A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:00<00:00, 40.58it/s, episode_len=13][A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:00<00:00, 40.58it/s, episode_len=1] [A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:00<00:00, 40.58it/s, episode_len=20][A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:00<00:00, 40.58it/s, episode_len=20][A

Collecting trajectories:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:00<00:00, 42.32it/s, episode_len=20][A

Collecting trajectories:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:00<00:00, 42.32it/s, episode_len=2] [A

Collecting trajectories:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:00<00:00, 42.32it/s,
...[truncated]
================================================================================
Trial 2 | time = 2025-11-15T23:59:11.237201
Command: python -m src.train_rl_ppo --config config_trial_002.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05  ->  0.0395
  - physchem_constraints.max_repeats: 2  ->  2
  - physchem_constraints.min_entropy: 3  ->  3.976
  - physchem_constraints.entropy_penalty_coef: 5.8  ->  6.481
  - reward_weights.physchem: 0.25  ->  0.2349
  - reward_weights.target_prob: 0.35  ->  0.3573
  - active_selection.ucb.diversity_weight: 0.1  ->  0.3088
  - rl_enhanced.kl_penalty.beta: 0.02  ->  0.007529

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/32 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/32 [00:00<?, ?it/s, episode_len=5][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:08,  3.72it/s, episode_len=5][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:08,  3.72it/s, episode_len=4][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:08,  3.72it/s, episode_len=4][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:08,  3.72it/s, episode_len=11][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:08,  3.72it/s, episode_len=20][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:08,  3.72it/s, episode_len=16][A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 18.32it/s, episode_len=16][A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 18.32it/s, episode_len=14][A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 18.32it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 18.32it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:01, 20.58it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:01, 20.58it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:01, 20.58it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:01, 20.58it/s, episode_len=5] [A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:00<00:00, 23.22it/s, episode_len=5][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:00<00:00, 23.22it/s, episode_len=1][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:00<00:00, 23.22it/s, episode_len=9][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:00<00:00, 23.22it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:00<00:00, 23.22it/s, episode_len=20][A

Collecting trajectories:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:00<00:00, 27.81it/s, episode_len=20][A

Collecting trajectories:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:00<00:00, 27.81it/s, episode_len=20][A

Collecting trajectories:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:00<00:00, 27.81it/s, episode_len=20][A

Collecting trajectories:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:00<00:00, 27.81it/s, episode_len=20][A

Collecting trajectories:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 16/32 [00:00<00:00, 27.81it/s, episode_len=20][A

Collecting trajectories:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:00<00:00, 26.85it/s, episode_len=20][A

Collecting trajectories:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:00<00:00, 26.85it/s, episode_len=8] [A

Collecting trajectories:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:00<00:00, 26.85it/s, episode_len=11][A

Collecting trajectories:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:00<00:00, 26.85it/s, episode_len=20][A

Collecting trajectories:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:00<00:00, 26.85it/s, episode_len=8] [A

Collecting trajectories:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 20/32 [00:01<00:00, 26.85it/s, episode_len=20][A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:01<00:00, 30.17it/s, episode_len=20][A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:01<00:00, 30.17it/s, episode_len=20][A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:01<00:00, 30.17it/s, episode_len=20][A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:01<00:00, 30.17it/s, episode_len=19][A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:01<00:00, 30.17it/s, episode_len=20][A

Collecting trajectories:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:01<00:00, 25.76it/s, episode_len=20][A

Collecting trajectories:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 29/32 [00:01<00:00, 25.76it/s, 
...[truncated]
================================================================================
Trial 3 | time = 2025-11-16T00:32:08.348554
Command: python -m src.train_rl_ppo --config config_trial_003.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05  ->  0.03689
  - physchem_constraints.max_repeats: 2  ->  2
  - physchem_constraints.min_entropy: 3  ->  2.579
  - physchem_constraints.entropy_penalty_coef: 5.8  ->  6.379
  - reward_weights.physchem: 0.25  ->  0.3459
  - reward_weights.target_prob: 0.35  ->  0.4689
  - active_selection.ucb.diversity_weight: 0.1  ->  0.1035
  - rl_enhanced.kl_penalty.beta: 0.02  ->  0.02676

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/32 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/32 [00:00<?, ?it/s, episode_len=19][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.20it/s, episode_len=19][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.20it/s, episode_len=20][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.20it/s, episode_len=16][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.20it/s, episode_len=12][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.20it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 5/32 [00:00<00:02, 13.14it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 5/32 [00:00<00:02, 13.14it/s, episode_len=10][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 5/32 [00:00<00:02, 13.14it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 5/32 [00:00<00:02, 13.14it/s, episode_len=10][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:00<00:01, 17.07it/s, episode_len=10][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:00<00:01, 17.07it/s, episode_len=10][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:00<00:01, 17.07it/s, episode_len=7] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:00<00:01, 17.07it/s, episode_len=3][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:00<00:01, 17.07it/s, episode_len=12][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 8/32 [00:00<00:01, 17.07it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 26.02it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 26.02it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 26.02it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 26.02it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 26.02it/s, episode_len=12][A

Collecting trajectories:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:00<00:00, 26.68it/s, episode_len=12][A

Collecting trajectories:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:00<00:00, 26.68it/s, episode_len=11][A

Collecting trajectories:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:00<00:00, 26.68it/s, episode_len=11][A

Collecting trajectories:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:00<00:00, 26.68it/s, episode_len=12][A

Collecting trajectories:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:00<00:00, 26.68it/s, episode_len=15][A

Collecting trajectories:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:00<00:00, 29.61it/s, episode_len=15][A

Collecting trajectories:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:00<00:00, 29.61it/s, episode_len=11][A

Collecting trajectories:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:00<00:00, 29.61it/s, episode_len=20][A

Collecting trajectories:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:01<00:00, 29.61it/s, episode_len=20][A

Collecting trajectories:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:01<00:00, 29.61it/s, episode_len=20][A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:01<00:00, 28.21it/s, episode_len=20][A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:01<00:00, 28.21it/s, episode_len=20][A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:01<00:00, 28.21it/s, episode_len=3] [A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:01<00:00, 28.21it/s, episode_len=1][A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:01<00:00, 28.21it/s, episode_len=20][A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:01<00:00, 28.21it/s, episode_len=16][A

Collecting trajectories:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:01<00:00, 31.46it
...[truncated]
================================================================================
Trial 4 | time = 2025-11-16T01:05:16.018699
Command: python -m src.train_rl_ppo --config config_trial_004.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05  ->  0.02766
  - physchem_constraints.max_repeats: 2  ->  3
  - physchem_constraints.min_entropy: 3  ->  2.198
  - physchem_constraints.entropy_penalty_coef: 5.8  ->  6.549
  - reward_weights.physchem: 0.25  ->  0.2391
  - reward_weights.target_prob: 0.35  ->  0.4548
  - active_selection.ucb.diversity_weight: 0.1  ->  0.3433
  - rl_enhanced.kl_penalty.beta: 0.02  ->  0.04708

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/32 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/32 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.11it/s, episode_len=20][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.11it/s, episode_len=20][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.11it/s, episode_len=19][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.11it/s, episode_len=2] [A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.11it/s, episode_len=16][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 5/32 [00:00<00:01, 13.95it/s, episode_len=16][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 5/32 [00:00<00:01, 13.95it/s, episode_len=5] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 5/32 [00:00<00:01, 13.95it/s, episode_len=19][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 5/32 [00:00<00:01, 13.95it/s, episode_len=10][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 5/32 [00:00<00:01, 13.95it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:01, 21.15it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:01, 21.15it/s, episode_len=5] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:01, 21.15it/s, episode_len=12][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:01, 21.15it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:01, 21.15it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 25.25it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 25.25it/s, episode_len=6] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 25.25it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 25.25it/s, episode_len=5] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 25.25it/s, episode_len=14][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 25.25it/s, episode_len=4] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 25.25it/s, episode_len=20][A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:00<00:00, 32.12it/s, episode_len=20][A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:00<00:00, 32.12it/s, episode_len=20][A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:00<00:00, 32.12it/s, episode_len=7] [A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:00<00:00, 32.12it/s, episode_len=16][A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:00<00:00, 32.12it/s, episode_len=11][A

Collecting trajectories:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:00<00:00, 33.56it/s, episode_len=11][A

Collecting trajectories:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:00<00:00, 33.56it/s, episode_len=13][A

Collecting trajectories:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:00<00:00, 33.56it/s, episode_len=20][A

Collecting trajectories:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:01<00:00, 33.56it/s, episode_len=19][A

Collecting trajectories:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:01<00:00, 33.56it/s, episode_len=10][A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 33.50it/s, episode_len=10][A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 33.50it/s, episode_len=14][A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 33.50it/s, episode_len=20][A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 33.50it/s, episode_len=20][A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 33.50
...[truncated]
================================================================================
Trial 5 | time = 2025-11-16T01:37:28.736350
Command: python -m src.train_rl_ppo --config config_trial_005.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05  ->  0.08235
  - physchem_constraints.max_repeats: 2  ->  2
  - physchem_constraints.min_entropy: 3  ->  3.541
  - physchem_constraints.entropy_penalty_coef: 5.8  ->  3.181
  - reward_weights.physchem: 0.25  ->  0.2928
  - reward_weights.target_prob: 0.35  ->  0.3815
  - active_selection.ucb.diversity_weight: 0.1  ->  0.3525
  - rl_enhanced.kl_penalty.beta: 0.02  ->  0.007072

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/32 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/32 [00:00<?, ?it/s, episode_len=19][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:10,  3.08it/s, episode_len=19][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:10,  3.08it/s, episode_len=7] [A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:10,  3.08it/s, episode_len=8][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:10,  3.08it/s, episode_len=7][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:10,  3.08it/s, episode_len=14][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:10,  3.08it/s, episode_len=10][A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 16.98it/s, episode_len=10][A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 16.98it/s, episode_len=5] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 16.98it/s, episode_len=4][A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 16.98it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 16.98it/s, episode_len=11][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:00<00:00, 23.69it/s, episode_len=11][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:00<00:00, 23.69it/s, episode_len=18][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:00<00:00, 23.69it/s, episode_len=1] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:00<00:00, 23.69it/s, episode_len=15][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:00<00:00, 23.69it/s, episode_len=3] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:00<00:00, 23.69it/s, episode_len=20][A

Collecting trajectories:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:00<00:00, 28.02it/s, episode_len=20][A

Collecting trajectories:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:00<00:00, 28.02it/s, episode_len=20][A

Collecting trajectories:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:00<00:00, 28.02it/s, episode_len=2] [A

Collecting trajectories:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:00<00:00, 28.02it/s, episode_len=20][A

Collecting trajectories:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:00<00:00, 28.02it/s, episode_len=4] [A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:00<00:00, 29.94it/s, episode_len=4][A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:00<00:00, 29.94it/s, episode_len=5][A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:00<00:00, 29.94it/s, episode_len=20][A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:00<00:00, 29.94it/s, episode_len=5] [A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:00<00:00, 29.94it/s, episode_len=12][A

Collecting trajectories:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:00<00:00, 31.92it/s, episode_len=12][A

Collecting trajectories:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:00<00:00, 31.92it/s, episode_len=11][A

Collecting trajectories:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:00<00:00, 31.92it/s, episode_len=15][A

Collecting trajectories:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:01<00:00, 31.92it/s, episode_len=20][A

Collecting trajectories:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:01<00:00, 31.92it/s, episode_len=15][A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 29.59it/s, episode_len=15][A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 29.59it/s, episode_len=2] [A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 29.59it/s, episode_len=1][A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 29.59it/s, episode_len=14][A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 29.59it
...[truncated]
================================================================================
Trial 6 | time = 2025-11-16T02:10:17.689878
Command: python -m src.train_rl_ppo --config config_trial_006.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05  ->  0.04426
  - physchem_constraints.max_repeats: 2  ->  4
  - physchem_constraints.min_entropy: 3  ->  2.848
  - physchem_constraints.entropy_penalty_coef: 5.8  ->  5.58
  - reward_weights.physchem: 0.25  ->  0.4299
  - reward_weights.target_prob: 0.35  ->  0.3769
  - active_selection.ucb.diversity_weight: 0.1  ->  0.2529
  - rl_enhanced.kl_penalty.beta: 0.02  ->  0.01205

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/32 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/32 [00:00<?, ?it/s, episode_len=4][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:08,  3.77it/s, episode_len=4][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:08,  3.77it/s, episode_len=3][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:08,  3.77it/s, episode_len=14][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:08,  3.77it/s, episode_len=20][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:08,  3.77it/s, episode_len=2] [A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:08,  3.77it/s, episode_len=6][A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 18.94it/s, episode_len=6][A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 18.94it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 18.94it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 18.94it/s, episode_len=3] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 18.94it/s, episode_len=10][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:00<00:00, 24.98it/s, episode_len=10][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:00<00:00, 24.98it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:00<00:00, 24.98it/s, episode_len=8] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:00<00:00, 24.98it/s, episode_len=12][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:00<00:00, 24.98it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:00<00:00, 27.26it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:00<00:00, 27.26it/s, episode_len=11][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:00<00:00, 27.26it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:00<00:00, 27.26it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/32 [00:00<00:00, 27.26it/s, episode_len=15][A

Collecting trajectories:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:00<00:00, 28.21it/s, episode_len=15][A

Collecting trajectories:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:00<00:00, 28.21it/s, episode_len=12][A

Collecting trajectories:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:00<00:00, 28.21it/s, episode_len=10][A

Collecting trajectories:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:00<00:00, 28.21it/s, episode_len=15][A

Collecting trajectories:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:00<00:00, 28.21it/s, episode_len=20][A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:00<00:00, 29.47it/s, episode_len=20][A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:00<00:00, 29.47it/s, episode_len=1] [A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:00<00:00, 29.47it/s, episode_len=9][A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:00<00:00, 29.47it/s, episode_len=6][A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:00<00:00, 29.47it/s, episode_len=20][A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:00<00:00, 29.47it/s, episode_len=15][A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:00<00:00, 34.21it/s, episode_len=15][A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 34.21it/s, episode_len=17][A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 34.21it/s, episode_len=14][A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 34.21it/s, episode_len=8] [A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 34.21it/
...[truncated]
================================================================================
Trial 7 | time = 2025-11-16T02:44:21.162020
Command: python -m src.train_rl_ppo --config config_trial_007.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05  ->  0.05202
  - physchem_constraints.max_repeats: 2  ->  2
  - physchem_constraints.min_entropy: 3  ->  2.054
  - physchem_constraints.entropy_penalty_coef: 5.8  ->  9.408
  - reward_weights.physchem: 0.25  ->  0.2874
  - reward_weights.target_prob: 0.35  ->  0.2647
  - active_selection.ucb.diversity_weight: 0.1  ->  0.3741
  - rl_enhanced.kl_penalty.beta: 0.02  ->  0.02044

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/32 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/32 [00:00<?, ?it/s, episode_len=2][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:08,  3.83it/s, episode_len=2][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:08,  3.83it/s, episode_len=2][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:08,  3.83it/s, episode_len=6][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:08,  3.83it/s, episode_len=6][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:08,  3.83it/s, episode_len=17][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:08,  3.83it/s, episode_len=16][A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 19.72it/s, episode_len=16][A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 19.72it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 19.72it/s, episode_len=18][A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 19.72it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:01, 21.84it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:01, 21.84it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:01, 21.84it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:01, 21.84it/s, episode_len=1] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:01, 21.84it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 25.93it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 25.93it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 25.93it/s, episode_len=1] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 25.93it/s, episode_len=12][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 25.93it/s, episode_len=20][A

Collecting trajectories:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:00<00:00, 28.53it/s, episode_len=20][A

Collecting trajectories:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:00<00:00, 28.53it/s, episode_len=20][A

Collecting trajectories:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:00<00:00, 28.53it/s, episode_len=20][A

Collecting trajectories:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:00<00:00, 28.53it/s, episode_len=12][A

Collecting trajectories:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:00<00:00, 28.53it/s, episode_len=12][A

Collecting trajectories:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:00<00:00, 28.50it/s, episode_len=12][A

Collecting trajectories:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:00<00:00, 28.50it/s, episode_len=12][A

Collecting trajectories:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:00<00:00, 28.50it/s, episode_len=12][A

Collecting trajectories:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:00<00:00, 28.50it/s, episode_len=15][A

Collecting trajectories:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 21/32 [00:00<00:00, 28.50it/s, episode_len=20][A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:00<00:00, 30.20it/s, episode_len=20][A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:00<00:00, 30.20it/s, episode_len=1] [A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:01<00:00, 30.20it/s, episode_len=19][A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:01<00:00, 30.20it/s, episode_len=8] [A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:01<00:00, 30.20it/s, episode_len=5][A

Collecting trajectories:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 25/32 [00:01<00:00, 30.20it/s, episode_len=20][A

Collecting trajectories:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:01<00:00, 34.86it/s, 
...[truncated]
================================================================================
Trial 8 | time = 2025-11-16T03:18:44.747723
Command: python -m src.train_rl_ppo --config config_trial_008.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05  ->  0.09972
  - physchem_constraints.max_repeats: 2  ->  3
  - physchem_constraints.min_entropy: 3  ->  3.763
  - physchem_constraints.entropy_penalty_coef: 5.8  ->  7.872
  - reward_weights.physchem: 0.25  ->  0.4279
  - reward_weights.target_prob: 0.35  ->  0.3652
  - active_selection.ucb.diversity_weight: 0.1  ->  0.1602
  - rl_enhanced.kl_penalty.beta: 0.02  ->  0.04077

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/32 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/32 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.18it/s, episode_len=20][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.18it/s, episode_len=4] [A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.18it/s, episode_len=7][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.18it/s, episode_len=20][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.18it/s, episode_len=3] [A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.18it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 17.23it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 17.23it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 17.23it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 6/32 [00:00<00:01, 17.23it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:01, 19.51it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:01, 19.51it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:01, 19.51it/s, episode_len=10][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:01, 19.51it/s, episode_len=14][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 9/32 [00:00<00:01, 19.51it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 23.24it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 23.24it/s, episode_len=2] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 23.24it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 23.24it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 23.24it/s, episode_len=20][A

Collecting trajectories:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:00<00:00, 25.22it/s, episode_len=20][A

Collecting trajectories:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:00<00:00, 25.22it/s, episode_len=20][A

Collecting trajectories:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:00<00:00, 25.22it/s, episode_len=20][A

Collecting trajectories:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:00<00:00, 25.22it/s, episode_len=1] [A

Collecting trajectories:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:00<00:00, 25.22it/s, episode_len=7][A

Collecting trajectories:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 17/32 [00:00<00:00, 25.22it/s, episode_len=20][A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:00<00:00, 28.53it/s, episode_len=20][A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:01<00:00, 28.53it/s, episode_len=20][A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:01<00:00, 28.53it/s, episode_len=20][A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:01<00:00, 28.53it/s, episode_len=13][A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:01<00:00, 28.53it/s, episode_len=20][A

Collecting trajectories:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:01<00:00, 28.06it/s, episode_len=20][A

Collecting trajectories:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:01<00:00, 28.06it/s, episode_len=14][A

Collecting trajectories:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:01<00:00, 28.06it/s, episode_len=20][A

Collecting trajectories:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:01<00:00, 28.06it/s, episode_len=7] [A

Collecting trajectories:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 26/32 [00:01<00:00, 28.06it/s, episode_len=11][A

Collecting trajectories:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 30/32 [00:01<00:00, 30.76it
...[truncated]
================================================================================
Trial 9 | time = 2025-11-16T03:51:44.961899
Command: python -m src.train_rl_ppo --config config_trial_009.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05  ->  0.03807
  - physchem_constraints.max_repeats: 2  ->  3
  - physchem_constraints.min_entropy: 3  ->  2.238
  - physchem_constraints.entropy_penalty_coef: 5.8  ->  3.678
  - reward_weights.physchem: 0.25  ->  0.4092
  - reward_weights.target_prob: 0.35  ->  0.2831
  - active_selection.ucb.diversity_weight: 0.1  ->  0.05213
  - rl_enhanced.kl_penalty.beta: 0.02  ->  0.02976

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/32 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/32 [00:00<?, ?it/s, episode_len=19][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.15it/s, episode_len=19][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.15it/s, episode_len=20][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.15it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 3/32 [00:00<00:03,  8.17it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 3/32 [00:00<00:03,  8.17it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 3/32 [00:00<00:03,  8.17it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 5/32 [00:00<00:02, 11.59it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 5/32 [00:00<00:02, 11.59it/s, episode_len=9] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 5/32 [00:00<00:02, 11.59it/s, episode_len=6][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 5/32 [00:00<00:02, 11.59it/s, episode_len=16][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 5/32 [00:00<00:02, 11.59it/s, episode_len=1] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 5/32 [00:00<00:02, 11.59it/s, episode_len=19][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:00<00:01, 21.57it/s, episode_len=19][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:00<00:01, 21.57it/s, episode_len=12][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:00<00:01, 21.57it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:00<00:01, 21.57it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 22.72it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 22.72it/s, episode_len=5] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 22.72it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 22.72it/s, episode_len=3] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 22.72it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 13/32 [00:00<00:00, 22.72it/s, episode_len=9] [A

Collecting trajectories:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:00<00:00, 29.28it/s, episode_len=9][A

Collecting trajectories:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:00<00:00, 29.28it/s, episode_len=14][A

Collecting trajectories:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:00<00:00, 29.28it/s, episode_len=6] [A

Collecting trajectories:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:01<00:00, 29.28it/s, episode_len=4][A

Collecting trajectories:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 18/32 [00:01<00:00, 29.28it/s, episode_len=20][A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:01<00:00, 27.67it/s, episode_len=20][A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:01<00:00, 27.67it/s, episode_len=9] [A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:01<00:00, 27.67it/s, episode_len=4][A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:01<00:00, 27.67it/s, episode_len=20][A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:01<00:00, 27.67it/s, episode_len=8] [A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 22/32 [00:01<00:00, 27.67it/s, episode_len=20][A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 31.07it/s, episode_len=20][A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 31.07it/s, episode_len=15][A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 31.07it/s, episode_len=20][A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 31.07i
...[truncated]
================================================================================
Trial 10 | time = 2025-11-16T04:24:26.066060
Command: python -m src.train_rl_ppo --config config_trial_010.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05  ->  0.07179
  - physchem_constraints.max_repeats: 2  ->  4
  - physchem_constraints.min_entropy: 3  ->  2.128
  - physchem_constraints.entropy_penalty_coef: 5.8  ->  7.638
  - reward_weights.physchem: 0.25  ->  0.2266
  - reward_weights.target_prob: 0.35  ->  0.4902
  - active_selection.ucb.diversity_weight: 0.1  ->  0.4912
  - rl_enhanced.kl_penalty.beta: 0.02  ->  0.03306

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/32 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/32 [00:00<?, ?it/s, episode_len=14][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.30it/s, episode_len=14][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.30it/s, episode_len=20][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.30it/s, episode_len=20][A

Collecting trajectories:   3%|â–Ž         | 1/32 [00:00<00:09,  3.30it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 4/32 [00:00<00:02, 11.07it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 4/32 [00:00<00:02, 11.07it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 4/32 [00:00<00:02, 11.07it/s, episode_len=7] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 4/32 [00:00<00:02, 11.07it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 7/32 [00:00<00:01, 15.57it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 7/32 [00:00<00:01, 15.57it/s, episode_len=12][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 7/32 [00:00<00:01, 15.57it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 7/32 [00:00<00:01, 15.57it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:00<00:01, 17.89it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:00<00:01, 17.89it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:00<00:01, 17.89it/s, episode_len=4] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:00<00:01, 17.89it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:00<00:01, 17.89it/s, episode_len=4] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 10/32 [00:00<00:01, 17.89it/s, episode_len=3][A

Collecting trajectories:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:00<00:00, 26.55it/s, episode_len=3][A

Collecting trajectories:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:00<00:00, 26.55it/s, episode_len=20][A

Collecting trajectories:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:00<00:00, 26.55it/s, episode_len=11][A

Collecting trajectories:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:00<00:00, 26.55it/s, episode_len=20][A

Collecting trajectories:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:00<00:00, 26.55it/s, episode_len=2] [A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:00<00:00, 29.13it/s, episode_len=2][A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:00<00:00, 29.13it/s, episode_len=20][A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:00<00:00, 29.13it/s, episode_len=20][A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:00<00:00, 29.13it/s, episode_len=4] [A

Collecting trajectories:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 19/32 [00:01<00:00, 29.13it/s, episode_len=6][A

Collecting trajectories:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:01<00:00, 32.07it/s, episode_len=6][A

Collecting trajectories:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:01<00:00, 32.07it/s, episode_len=4][A

Collecting trajectories:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:01<00:00, 32.07it/s, episode_len=8][A

Collecting trajectories:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:01<00:00, 32.07it/s, episode_len=4][A

Collecting trajectories:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 23/32 [00:01<00:00, 32.07it/s, episode_len=20][A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 33.17it/s, episode_len=20][A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 33.17it/s, episode_len=13][A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 33.17it/s, episode_len=10][A

Collecting trajectories:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/32 [00:01<00:00, 33.17it/s,
...[truncated]
================================================================================
Trial 1 | time = 2025-11-16T10:22:39.961975
Command: python -m src.train_rl_ppo --config config_trial_001.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.02685
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  3.507
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  6.98
  - reward_weights.physchem: 0.4328  ->  0.483
  - reward_weights.target_prob: 0.468  ->  0.4065
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2036
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.02792

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.02685, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=19][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=3] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=3][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.10it/s, episode_len=3][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.10it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.10it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.10it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 16.75it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 16.75it/s, episode_len=2] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 16.75it/s, episode_len=4][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 16.75it/s, episode_len=3][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 16.75it/s, episode_len=4][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 16.75it/s, episode_len=1][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 16.75it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 16.75it/s, episode_len=6] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 16.75it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.68it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.68it/s, episode_len=15][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.68it/s, episode_len=3] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.68it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.68it/s, episode_len=12][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 31.46it/s, episode_len=12][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 31.46it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 31.46it/s, episode_len=4] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 31.46it/s, episode_len=4][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 31.46it/s, episode_len=4][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 31.46it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 34.06it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 34.06it/s, episode_len=7] [A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 34.06it/s, episode_len=4][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 34.06it/s, episode_len=3][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 34.06it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 34.06it/s, episode_len=1
...[truncated]
================================================================================
Trial 2 | time = 2025-11-16T10:58:54.475858
Command: python -m src.train_rl_ppo --config config_trial_002.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.06153
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  3.992
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  3.499
  - reward_weights.physchem: 0.4328  ->  0.2745
  - reward_weights.target_prob: 0.468  ->  0.4126
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.4308
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.00557

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.06153, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.01it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.01it/s, episode_len=11][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.01it/s, episode_len=5] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.01it/s, episode_len=9][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.01it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.01it/s, episode_len=16][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.16it/s, episode_len=16][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.16it/s, episode_len=11][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.16it/s, episode_len=13][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.16it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.16it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 21.39it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 21.39it/s, episode_len=4] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 21.39it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 21.39it/s, episode_len=3] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 21.39it/s, episode_len=13][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 21.39it/s, episode_len=15][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 28.57it/s, episode_len=15][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 28.57it/s, episode_len=7] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 28.57it/s, episode_len=5][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 28.57it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 28.57it/s, episode_len=9] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 28.57it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 32.79it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 32.79it/s, episode_len=9] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 32.79it/s, episode_len=6][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 32.79it/s, episode_len=9][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 32.79it/s, episode_len=5][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 32.79it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 32.79it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:01, 36.73it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:01, 36.73it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 36.73it/s, episode_len=10][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 36.73it/s, episode_len=9] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 36.73it/s, e
...[truncated]
================================================================================
Trial 3 | time = 2025-11-16T11:34:42.991486
Command: python -m src.train_rl_ppo --config config_trial_003.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.09903
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  3.375
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  9.416
  - reward_weights.physchem: 0.4328  ->  0.3277
  - reward_weights.target_prob: 0.468  ->  0.3435
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3361
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.04352

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.09903, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.31it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.31it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.31it/s, episode_len=5] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.31it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.31it/s, episode_len=3] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.31it/s, episode_len=12][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.43it/s, episode_len=12][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.43it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.43it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.43it/s, episode_len=14][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.99it/s, episode_len=14][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.99it/s, episode_len=3] [A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.99it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.99it/s, episode_len=4] [A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.99it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.99it/s, episode_len=1] [A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.99it/s, episode_len=7][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 32.15it/s, episode_len=7][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 32.15it/s, episode_len=1][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 32.15it/s, episode_len=3][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 32.15it/s, episode_len=7][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 32.15it/s, episode_len=3][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 32.15it/s, episode_len=2][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 32.15it/s, episode_len=13][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 32.15it/s, episode_len=19][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 32.15it/s, episode_len=5] [A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:00, 46.19it/s, episode_len=5][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:00, 46.19it/s, episode_len=11][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:00, 46.19it/s, episode_len=19][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:00, 46.19it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:00, 46.19it/s, episode_len=5] [A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:00, 46.19it/s, episode_len=3][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:00, 46.19it/s, episode_len=4][A

Collecting trajectories:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/64 [00:00<00:00, 47.34it/s, episode_len=4][A

Collecting trajectories:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/64 [00:00<00:00, 47.34it/s, episode_len=1
...[truncated]
================================================================================
Trial 4 | time = 2025-11-16T12:10:55.135825
Command: python -m src.train_rl_ppo --config config_trial_004.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.02733
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  2.754
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  3.198
  - reward_weights.physchem: 0.4328  ->  0.2849
  - reward_weights.target_prob: 0.468  ->  0.4515
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3433
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.009555

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.02733, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=16][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.23it/s, episode_len=16][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.23it/s, episode_len=2] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.23it/s, episode_len=17][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.23it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.23it/s, episode_len=9] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.23it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.27it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.27it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.27it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.27it/s, episode_len=4] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.27it/s, episode_len=11][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.65it/s, episode_len=11][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.65it/s, episode_len=4] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.65it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.65it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.65it/s, episode_len=9] [A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.38it/s, episode_len=9][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.38it/s, episode_len=18][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.38it/s, episode_len=19][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.38it/s, episode_len=10][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.38it/s, episode_len=8] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 30.40it/s, episode_len=8][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 30.40it/s, episode_len=11][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 30.40it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 30.40it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 30.40it/s, episode_len=16][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 30.47it/s, episode_len=16][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 30.47it/s, episode_len=19][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 30.47it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 30.47it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 30.47it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 28.75it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 28.75it/s, episode_len=16][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 28.75it/s, episode_len=4] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 28.75it/s
...[truncated]
================================================================================
Trial 5 | time = 2025-11-16T12:46:57.058002
Command: python -m src.train_rl_ppo --config config_trial_005.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.05874
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  3.42
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  7.84
  - reward_weights.physchem: 0.4328  ->  0.3825
  - reward_weights.target_prob: 0.468  ->  0.2969
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3165
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.04854

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.05874, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=8][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.68it/s, episode_len=8][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.68it/s, episode_len=16][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.68it/s, episode_len=18][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.68it/s, episode_len=3] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.68it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.77it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.77it/s, episode_len=8] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.77it/s, episode_len=2][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.77it/s, episode_len=7][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.77it/s, episode_len=7][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.77it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.77it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:01, 26.85it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:01, 26.85it/s, episode_len=15][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:01, 26.85it/s, episode_len=7] [A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:01, 26.85it/s, episode_len=6][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:01, 26.85it/s, episode_len=17][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:01, 26.85it/s, episode_len=15][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 31.96it/s, episode_len=15][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 31.96it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 31.96it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 31.96it/s, episode_len=19][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 31.96it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 29.80it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 29.80it/s, episode_len=2] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 29.80it/s, episode_len=17][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 29.80it/s, episode_len=7] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 29.80it/s, episode_len=13][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 29.80it/s, episode_len=19][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 33.96it/s, episode_len=19][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 33.96it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 33.96it/s, episode_len=12][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 33.96it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 33.96it/s, episode_len=9] [A

Collecting trajectories:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/64 [00:01<00:01, 33.84it/s, epi
...[truncated]
================================================================================
Trial 6 | time = 2025-11-16T13:23:11.359921
Command: python -m src.train_rl_ppo --config config_trial_006.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.03337
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  3.179
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  6.049
  - reward_weights.physchem: 0.4328  ->  0.3847
  - reward_weights.target_prob: 0.468  ->  0.2625
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2574
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.04506

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.03337, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=11][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.33it/s, episode_len=11][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.33it/s, episode_len=16][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.33it/s, episode_len=17][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.33it/s, episode_len=11][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.33it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.91it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.91it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.91it/s, episode_len=10][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.91it/s, episode_len=6] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.91it/s, episode_len=14][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.91it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.05it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.05it/s, episode_len=14][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.05it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.05it/s, episode_len=8] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.05it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 25.42it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 25.42it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 25.42it/s, episode_len=6] [A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 25.42it/s, episode_len=11][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 25.42it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.38it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.38it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.38it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.38it/s, episode_len=19][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.38it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 27.40it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 27.40it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 27.40it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 27.40it/s, episode_len=2] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 27.40it/s, episode_len=3][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 27.40it/s, episode_len=9][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 32.53it/s, episode_len=9][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 32.53it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 32.53it/s,
...[truncated]
================================================================================
Trial 7 | time = 2025-11-16T13:59:34.401435
Command: python -m src.train_rl_ppo --config config_trial_007.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.03766
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  2.023
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  9.309
  - reward_weights.physchem: 0.4328  ->  0.4693
  - reward_weights.target_prob: 0.468  ->  0.3497
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2426
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03692

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.03766, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=19][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=7] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.11it/s, episode_len=7][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.11it/s, episode_len=1][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.11it/s, episode_len=1][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.11it/s, episode_len=3][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.11it/s, episode_len=11][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.11it/s, episode_len=7] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.11it/s, episode_len=8][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.11it/s, episode_len=1][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.11it/s, episode_len=9][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.11it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 30.49it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 30.49it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 30.49it/s, episode_len=10][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 30.49it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 30.49it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 28.02it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 28.02it/s, episode_len=12][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 28.02it/s, episode_len=3] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 28.02it/s, episode_len=13][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 28.02it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 29.91it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 29.91it/s, episode_len=9] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 29.91it/s, episode_len=4][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 29.91it/s, episode_len=5][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 29.91it/s, episode_len=15][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 29.91it/s, episode_len=15][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:01, 33.76it/s, episode_len=15][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:01, 33.76it/s, episode_len=9] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 33.76it/s, episode_len=10][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 33.76it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 33.76it/s, episode_
...[truncated]
================================================================================
Trial 8 | time = 2025-11-16T14:38:50.476474
Command: python -m src.train_rl_ppo --config config_trial_008.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.01358
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  3.769
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  7.517
  - reward_weights.physchem: 0.4328  ->  0.2948
  - reward_weights.target_prob: 0.468  ->  0.367
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2486
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03216

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.01358, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=8][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.60it/s, episode_len=8][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.60it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.60it/s, episode_len=7] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.60it/s, episode_len=7][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.60it/s, episode_len=2][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.60it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.67it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.67it/s, episode_len=3] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.67it/s, episode_len=1][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.67it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.67it/s, episode_len=13][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.67it/s, episode_len=4] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.67it/s, episode_len=4][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.67it/s, episode_len=2][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.67it/s, episode_len=18][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 33.71it/s, episode_len=18][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 33.71it/s, episode_len=9] [A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 33.71it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 33.71it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 33.71it/s, episode_len=6] [A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 33.71it/s, episode_len=8][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 35.71it/s, episode_len=8][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 35.71it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 35.71it/s, episode_len=9] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 35.71it/s, episode_len=1][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 35.71it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 35.71it/s, episode_len=4] [A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 38.75it/s, episode_len=4][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 38.75it/s, episode_len=18][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 38.75it/s, episode_len=17][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 38.75it/s, episode_len=5] [A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 38.75it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 38.75it/s, episode_len=13][A

Collecting trajectories:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/64 [00:00<00:00, 37.09it/s, episode_len=13][A

Collecting trajectories:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/64 [00:00<00:00, 37.09it/s, episode_len
...[truncated]
================================================================================
Trial 9 | time = 2025-11-16T15:14:59.678824
Command: python -m src.train_rl_ppo --config config_trial_009.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.05464
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  2.63
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  5.462
  - reward_weights.physchem: 0.4328  ->  0.3731
  - reward_weights.target_prob: 0.468  ->  0.4819
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1575
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03097

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.05464, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=9][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.42it/s, episode_len=9][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.42it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.42it/s, episode_len=16][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.42it/s, episode_len=14][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.42it/s, episode_len=12][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.43it/s, episode_len=12][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.43it/s, episode_len=11][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.43it/s, episode_len=15][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.43it/s, episode_len=13][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.43it/s, episode_len=8] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.43it/s, episode_len=9][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.55it/s, episode_len=9][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.55it/s, episode_len=2][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.55it/s, episode_len=3][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.55it/s, episode_len=15][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.55it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.55it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.63it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.63it/s, episode_len=14][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.63it/s, episode_len=1] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.63it/s, episode_len=4][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.63it/s, episode_len=7][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.63it/s, episode_len=16][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.63it/s, episode_len=1] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.63it/s, episode_len=14][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 40.78it/s, episode_len=14][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 40.78it/s, episode_len=13][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 40.78it/s, episode_len=12][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 40.78it/s, episode_len=9] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 40.78it/s, episode_len=12][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 40.78it/s, episode_len=7] [A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 42.93it/s, episode_len=7][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 42.93it/s, episode_len=8][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 42.93it/s, episode_len=2][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 42.93it/s, episode
...[truncated]
================================================================================
Trial 10 | time = 2025-11-16T15:50:46.043694
Command: python -m src.train_rl_ppo --config config_trial_010.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.09228
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  3.317
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  7.824
  - reward_weights.physchem: 0.4328  ->  0.4858
  - reward_weights.target_prob: 0.468  ->  0.3604
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3921
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.04687

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.09228, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:06, 10.00it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:06, 10.00it/s, episode_len=14][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:06, 10.00it/s, episode_len=14][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:06, 10.00it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 14.97it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 14.97it/s, episode_len=11][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 14.97it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 14.97it/s, episode_len=4] [A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 14.97it/s, episode_len=5][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 14.97it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.13it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.13it/s, episode_len=9] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.13it/s, episode_len=17][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.13it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:02, 23.60it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:02, 23.60it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:02, 23.60it/s, episode_len=4] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:02, 23.60it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 25.09it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 25.09it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:01<00:01, 25.09it/s, episode_len=18][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:01<00:01, 25.09it/s, episode_len=14][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 24.88it/s, episode_len=14][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 24.88it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 24.88it/s, episode_len=3] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 24.88it/s, episode_len=4][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 24.88it/s, episode_len=1][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 24.88it/s, episode_len=6][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 24.88it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 32.00it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 32.00it/s, epi
...[truncated]
================================================================================
Trial 11 | time = 2025-11-16T16:28:18.349613
Command: python -m src.train_rl_ppo --config config_trial_011.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.06364
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  2.476
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  7.438
  - reward_weights.physchem: 0.4328  ->  0.4504
  - reward_weights.target_prob: 0.468  ->  0.459
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3352
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.02144

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.06364, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.23it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.23it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.23it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.23it/s, episode_len=11][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.23it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.30it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.30it/s, episode_len=9] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.30it/s, episode_len=4][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.30it/s, episode_len=9][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.30it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.30it/s, episode_len=14][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.25it/s, episode_len=14][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.25it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.25it/s, episode_len=1] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.25it/s, episode_len=8][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.25it/s, episode_len=3][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.25it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.25it/s, episode_len=7] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 32.64it/s, episode_len=7][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 32.64it/s, episode_len=1][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 32.64it/s, episode_len=15][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 32.64it/s, episode_len=7] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 32.64it/s, episode_len=9][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 32.64it/s, episode_len=8][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 32.64it/s, episode_len=17][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 39.31it/s, episode_len=17][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 39.31it/s, episode_len=19][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 39.31it/s, episode_len=2] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 39.31it/s, episode_len=14][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 39.31it/s, episode_len=2] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 39.31it/s, episode_len=5][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 39.31it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:00<00:00, 42.88it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:00<00:00, 42.88it/s, episode_len=6] [A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:00<00:00, 42.88it/s, episo
...[truncated]
================================================================================
Trial 12 | time = 2025-11-16T17:04:29.711442
Command: python -m src.train_rl_ppo --config config_trial_012.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.04211
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  2.533
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  4.647
  - reward_weights.physchem: 0.4328  ->  0.305
  - reward_weights.target_prob: 0.468  ->  0.4768
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1861
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03218

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.04211, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=8][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.59it/s, episode_len=8][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.59it/s, episode_len=8][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.59it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.59it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.47it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.47it/s, episode_len=12][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.47it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.47it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 16.11it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 16.11it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 16.11it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 16.11it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 18.09it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 18.09it/s, episode_len=15][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 18.09it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 18.09it/s, episode_len=19][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:02, 19.28it/s, episode_len=19][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:02, 19.28it/s, episode_len=4] [A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:02, 19.28it/s, episode_len=16][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:02, 19.28it/s, episode_len=13][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:02, 19.28it/s, episode_len=8] [A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:02, 19.28it/s, episode_len=10][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 25.88it/s, episode_len=10][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 25.88it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:01<00:01, 25.88it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:01<00:01, 25.88it/s, episode_len=3] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 26.92it/s, episode_len=3][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 26.92it/s, episode_len=3][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 26.92it/s, episode_len=5][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 26.92it/s, episode_len=9][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 26.92it/s, episode_len=7][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 26.92it/s, episode_len=3][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 26.92it/s, episode_len=2][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 26.92it/s, episode_len=9][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 26.92it/s, episode_
...[truncated]
================================================================================
Trial 13 | time = 2025-11-16T17:40:01.163312
Command: python -m src.train_rl_ppo --config config_trial_013.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.04593
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  2.533
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  7.079
  - reward_weights.physchem: 0.4328  ->  0.3076
  - reward_weights.target_prob: 0.468  ->  0.3294
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.444
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01571

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.04593, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=9] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=18][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.18it/s, episode_len=18][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.18it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.18it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.18it/s, episode_len=1] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.18it/s, episode_len=16][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.03it/s, episode_len=16][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.03it/s, episode_len=5] [A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.03it/s, episode_len=2][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.03it/s, episode_len=3][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.03it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.03it/s, episode_len=16][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.03it/s, episode_len=3] [A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.03it/s, episode_len=4][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 33.72it/s, episode_len=4][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 33.72it/s, episode_len=3][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 33.72it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 33.72it/s, episode_len=3] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 33.72it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 33.72it/s, episode_len=7] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 37.99it/s, episode_len=7][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 37.99it/s, episode_len=15][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 37.99it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 37.99it/s, episode_len=8] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 37.99it/s, episode_len=9][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 37.99it/s, episode_len=6][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 39.81it/s, episode_len=6][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 39.81it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 39.81it/s, episode_len=14][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 39.81it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 39.81it/s, episode_len=
...[truncated]
================================================================================
Trial 14 | time = 2025-11-16T18:15:53.362953
Command: python -m src.train_rl_ppo --config config_trial_014.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.09596
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  3.539
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  4.625
  - reward_weights.physchem: 0.4328  ->  0.3454
  - reward_weights.target_prob: 0.468  ->  0.3339
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.239
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.04257

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.09596, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=1][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.73it/s, episode_len=1][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.73it/s, episode_len=14][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.73it/s, episode_len=14][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.73it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:04, 12.31it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:04, 12.31it/s, episode_len=10][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:04, 12.31it/s, episode_len=10][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:04, 12.31it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:04, 12.31it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 20.04it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 20.04it/s, episode_len=10][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 20.04it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 20.04it/s, episode_len=5] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 20.04it/s, episode_len=12][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 20.04it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 26.53it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 26.53it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 26.53it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 26.53it/s, episode_len=6] [A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 26.53it/s, episode_len=6][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 30.24it/s, episode_len=6][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 30.24it/s, episode_len=5][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 30.24it/s, episode_len=11][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 30.24it/s, episode_len=14][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 30.24it/s, episode_len=6] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 30.24it/s, episode_len=5][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 30.24it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 36.69it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 36.69it/s, episode_len=2] [A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 36.69it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 36.69it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 36.69it/s, episode_len=16][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:01, 36.18it/s, episode_len=16][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 36.18it/s, episode_len=18][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 36.18it/s, episod
...[truncated]
================================================================================
Trial 15 | time = 2025-11-16T18:51:02.383003
Command: python -m src.train_rl_ppo --config config_trial_015.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.01378
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  3.733
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  6.745
  - reward_weights.physchem: 0.4328  ->  0.3791
  - reward_weights.target_prob: 0.468  ->  0.4557
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1946
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.04897

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.01378, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=2][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.83it/s, episode_len=2][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.83it/s, episode_len=7][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.83it/s, episode_len=8][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.83it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.83it/s, episode_len=5] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.83it/s, episode_len=2][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.83it/s, episode_len=2][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.83it/s, episode_len=4][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 26.76it/s, episode_len=4][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 26.76it/s, episode_len=6][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 26.76it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 26.76it/s, episode_len=14][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 26.76it/s, episode_len=7] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 26.76it/s, episode_len=2][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 26.76it/s, episode_len=1][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 26.76it/s, episode_len=14][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 38.36it/s, episode_len=14][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 38.36it/s, episode_len=1] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 38.36it/s, episode_len=4][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 38.36it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 38.36it/s, episode_len=6] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 38.36it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 38.36it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 41.00it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 41.00it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 41.00it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 41.00it/s, episode_len=16][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 41.00it/s, episode_len=2] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 41.00it/s, episode_len=15][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 39.45it/s, episode_len=15][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 39.45it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 39.45it/s, episode_len=5] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 39.45it/s, episode_len=1][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 39.45it/s, episode_len=10][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 39.45it/s, episode_len=20
...[truncated]
================================================================================
Trial 16 | time = 2025-11-16T19:26:25.089542
Command: python -m src.train_rl_ppo --config config_trial_016.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.09055
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  3.857
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  8.34
  - reward_weights.physchem: 0.4328  ->  0.4392
  - reward_weights.target_prob: 0.468  ->  0.3535
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2933
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.0475

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.09055, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=16][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=14][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=3] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.57it/s, episode_len=3][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.57it/s, episode_len=19][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.57it/s, episode_len=8] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.57it/s, episode_len=2][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.57it/s, episode_len=17][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.57it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.27it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.27it/s, episode_len=17][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.27it/s, episode_len=5] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.27it/s, episode_len=1][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.27it/s, episode_len=12][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.27it/s, episode_len=1] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.27it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 32.80it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 32.80it/s, episode_len=8] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 32.80it/s, episode_len=10][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 32.80it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 32.80it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 33.56it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 33.56it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 33.56it/s, episode_len=6] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 33.56it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 33.56it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 32.74it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 32.74it/s, episode_len=5] [A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 32.74it/s, episode_len=10][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 32.74it/s, episode_len=17][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 32.74it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 34.47it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 34.47it/s,
...[truncated]
================================================================================
Trial 17 | time = 2025-11-16T20:02:02.836502
Command: python -m src.train_rl_ppo --config config_trial_017.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.08049
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  2.762
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  3.009
  - reward_weights.physchem: 0.4328  ->  0.477
  - reward_weights.target_prob: 0.468  ->  0.4915
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1629
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.04698

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.08049, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=10][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.23it/s, episode_len=10][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.23it/s, episode_len=2] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.23it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.23it/s, episode_len=17][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.23it/s, episode_len=15][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.54it/s, episode_len=15][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.54it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.54it/s, episode_len=7] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.54it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.54it/s, episode_len=3] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.54it/s, episode_len=17][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.20it/s, episode_len=17][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.20it/s, episode_len=6] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.20it/s, episode_len=2][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.20it/s, episode_len=13][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.20it/s, episode_len=1] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.20it/s, episode_len=14][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.20it/s, episode_len=3] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.20it/s, episode_len=9][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.20it/s, episode_len=1][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.20it/s, episode_len=6][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 41.17it/s, episode_len=6][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 41.17it/s, episode_len=3][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 41.17it/s, episode_len=10][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 41.17it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 41.17it/s, episode_len=9] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 41.17it/s, episode_len=3][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 41.17it/s, episode_len=5][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 41.17it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 44.49it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 44.49it/s, episode_len=10][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 44.49it/s, episode_len=3] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 44.49it/s, episode_len=4][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 44.49it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 44.49it/s, episo
...[truncated]
================================================================================
Trial 18 | time = 2025-11-16T20:39:52.286538
Command: python -m src.train_rl_ppo --config config_trial_018.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.0636
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  2.953
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  7.82
  - reward_weights.physchem: 0.4328  ->  0.245
  - reward_weights.target_prob: 0.468  ->  0.4334
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2977
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03936

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.0636, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=1][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.84it/s, episode_len=1][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.84it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.84it/s, episode_len=1] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.84it/s, episode_len=2][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.84it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.84it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 18.13it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 18.13it/s, episode_len=9] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 18.13it/s, episode_len=9][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 18.13it/s, episode_len=9][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 18.13it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 18.13it/s, episode_len=10][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:01, 26.81it/s, episode_len=10][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:01, 26.81it/s, episode_len=16][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:01, 26.81it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:01, 26.81it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:01, 26.81it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 26.82it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 26.82it/s, episode_len=14][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 26.82it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 26.82it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 26.82it/s, episode_len=8] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.75it/s, episode_len=8][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.75it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.75it/s, episode_len=5] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.75it/s, episode_len=5][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.75it/s, episode_len=2][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.75it/s, episode_len=12][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.75it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 34.78it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 34.78it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 34.78it/s, episode_len=10][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 34.78it/s, episode_len=2] [A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 34.78it/s, episode_len=15][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 34.78it/s, episod
...[truncated]
================================================================================
Trial 19 | time = 2025-11-16T21:17:07.776599
Command: python -m src.train_rl_ppo --config config_trial_019.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.02182
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  2.841
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  8.312
  - reward_weights.physchem: 0.4328  ->  0.2674
  - reward_weights.target_prob: 0.468  ->  0.4677
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3943
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01507

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.02182, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=19][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=13][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=10][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.44it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.44it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.44it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.44it/s, episode_len=12][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.08it/s, episode_len=12][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.08it/s, episode_len=14][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.08it/s, episode_len=1] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.08it/s, episode_len=17][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.08it/s, episode_len=12][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.08it/s, episode_len=14][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 26.05it/s, episode_len=14][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 26.05it/s, episode_len=9] [A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 26.05it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 26.05it/s, episode_len=2] [A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 26.05it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.93it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.93it/s, episode_len=1] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.93it/s, episode_len=8][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.93it/s, episode_len=2][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.93it/s, episode_len=17][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.93it/s, episode_len=9] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.93it/s, episode_len=14][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 38.45it/s, episode_len=14][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 38.45it/s, episode_len=5] [A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 38.45it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 38.45it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 38.45it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:01<00:01, 38.45it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 34.96it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 34.96it/s, ep
...[truncated]
================================================================================
Trial 20 | time = 2025-11-16T21:53:54.110699
Command: python -m src.train_rl_ppo --config config_trial_020.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.04349
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  3.829
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  9.079
  - reward_weights.physchem: 0.4328  ->  0.2794
  - reward_weights.target_prob: 0.468  ->  0.424
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2301
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.0151

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.04349, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.05it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.05it/s, episode_len=12][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.05it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.05it/s, episode_len=6] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.05it/s, episode_len=18][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.86it/s, episode_len=18][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.86it/s, episode_len=5] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.86it/s, episode_len=8][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.86it/s, episode_len=12][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.86it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.86it/s, episode_len=4] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.86it/s, episode_len=2][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.86it/s, episode_len=16][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 27.95it/s, episode_len=16][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 27.95it/s, episode_len=3] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 27.95it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 27.95it/s, episode_len=12][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 27.95it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.41it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.41it/s, episode_len=1] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.41it/s, episode_len=3][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.41it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.41it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.41it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 33.53it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 33.53it/s, episode_len=10][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 33.53it/s, episode_len=3] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 33.53it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 33.53it/s, episode_len=3] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 33.53it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:01, 36.99it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:01, 36.99it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:01, 36.99it/s, episode_len=14][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 36.99it/s, episode_len=15][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 36.99it/s, e
...[truncated]
================================================================================
Trial 21 | time = 2025-11-16T22:29:12.746493
Command: python -m src.train_rl_ppo --config config_trial_021.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.02287
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  2.983
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  5.358
  - reward_weights.physchem: 0.4328  ->  0.4998
  - reward_weights.target_prob: 0.468  ->  0.2829
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1024
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01578

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.02287, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=16][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.26it/s, episode_len=16][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.26it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.26it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.26it/s, episode_len=16][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.42it/s, episode_len=16][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.42it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.42it/s, episode_len=10][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.42it/s, episode_len=4] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.42it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.84it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.84it/s, episode_len=2] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.84it/s, episode_len=8][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.84it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.84it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.84it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 26.11it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 26.11it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 26.11it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 26.11it/s, episode_len=13][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 26.11it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 26.76it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 26.76it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 26.76it/s, episode_len=19][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 26.76it/s, episode_len=12][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 26.76it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 27.49it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 27.49it/s, episode_len=2] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 27.49it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 27.49it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 27.49it/s, episode_len=14][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 30.00it/s, episode_len=14][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 30.00it/s, episode_len=10][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 30.00it/s, episode_len=4] [A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 30.00it/s, episode_len=14][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 30.00it/s, e
...[truncated]
================================================================================
Trial 22 | time = 2025-11-16T23:04:26.765482
Command: python -m src.train_rl_ppo --config config_trial_022.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.08389
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  3.259
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  8.114
  - reward_weights.physchem: 0.4328  ->  0.211
  - reward_weights.target_prob: 0.468  ->  0.2748
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.4758
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.02508

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.08389, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=9][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.36it/s, episode_len=9][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.36it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.36it/s, episode_len=7] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.36it/s, episode_len=7][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.36it/s, episode_len=18][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 15.08it/s, episode_len=18][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 15.08it/s, episode_len=13][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 15.08it/s, episode_len=13][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 15.08it/s, episode_len=7] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 15.08it/s, episode_len=3][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 15.08it/s, episode_len=7][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 15.08it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:01, 26.96it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:01, 26.96it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:01, 26.96it/s, episode_len=4] [A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:01, 26.96it/s, episode_len=5][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:01, 26.96it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:01, 26.96it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.46it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.46it/s, episode_len=3] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.46it/s, episode_len=12][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.46it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.46it/s, episode_len=19][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 32.40it/s, episode_len=19][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 32.40it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 32.40it/s, episode_len=5] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 32.40it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 32.40it/s, episode_len=5] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 32.40it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 33.74it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 33.74it/s, episode_len=11][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 33.74it/s, episode_len=14][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 33.74it/s, episode_len=18][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 33.74it/s, episode_len=9] [A

Collecting trajectories:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/64 [00:01<00:01, 34.80it/s, epi
...[truncated]
================================================================================
Trial 23 | time = 2025-11-16T23:40:52.901550
Command: python -m src.train_rl_ppo --config config_trial_023.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.08604
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  3.809
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  8.135
  - reward_weights.physchem: 0.4328  ->  0.461
  - reward_weights.target_prob: 0.468  ->  0.4217
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.141
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.0261

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.08604, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=8][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.45it/s, episode_len=8][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.45it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.45it/s, episode_len=13][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.45it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.17it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.17it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.17it/s, episode_len=10][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.17it/s, episode_len=4] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.17it/s, episode_len=15][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.76it/s, episode_len=15][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.76it/s, episode_len=8] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.76it/s, episode_len=12][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.76it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.76it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 24.16it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 24.16it/s, episode_len=2] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 24.16it/s, episode_len=4][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 24.16it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 24.16it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 24.16it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 28.57it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 28.57it/s, episode_len=14][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 28.57it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 28.57it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 28.57it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 24.62it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 24.62it/s, episode_len=19][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 24.62it/s, episode_len=17][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 24.62it/s, episode_len=2] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 24.62it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 25.42it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 25.42it/s, episode_len=2] [A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 25.42it/s, episode_len=4][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 25.42it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 25.42it/s, epi
...[truncated]
================================================================================
Trial 24 | time = 2025-11-17T00:19:04.723837
Command: python -m src.train_rl_ppo --config config_trial_024.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.05411
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  3.217
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  8.028
  - reward_weights.physchem: 0.4328  ->  0.394
  - reward_weights.target_prob: 0.468  ->  0.4477
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2497
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.0312

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.05411, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:21,  2.94it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:21,  2.94it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:21,  2.94it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:21,  2.94it/s, episode_len=18][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.62it/s, episode_len=18][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.62it/s, episode_len=2] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.62it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.62it/s, episode_len=1] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.62it/s, episode_len=12][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.62it/s, episode_len=12][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.62it/s, episode_len=10][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.60it/s, episode_len=10][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.60it/s, episode_len=5] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.60it/s, episode_len=14][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.60it/s, episode_len=3] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.60it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.60it/s, episode_len=3] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.60it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.74it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.74it/s, episode_len=3] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.74it/s, episode_len=9][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.74it/s, episode_len=2][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.74it/s, episode_len=2][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.74it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.74it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 36.94it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 36.94it/s, episode_len=15][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 36.94it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 36.94it/s, episode_len=9] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 36.94it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 36.94it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:01, 33.59it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 33.59it/s, episode_len=12][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 33.59it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 33.59it/s,
...[truncated]
================================================================================
Trial 25 | time = 2025-11-17T00:53:44.112765
Command: python -m src.train_rl_ppo --config config_trial_025.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.01486
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  2.548
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  8.488
  - reward_weights.physchem: 0.4328  ->  0.4479
  - reward_weights.target_prob: 0.468  ->  0.4544
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.365
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03607

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.01486, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.03it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.03it/s, episode_len=11][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.03it/s, episode_len=8] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.03it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.03it/s, episode_len=16][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.94it/s, episode_len=16][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.94it/s, episode_len=16][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.94it/s, episode_len=3] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.94it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.94it/s, episode_len=8] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.94it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.78it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.78it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.78it/s, episode_len=4] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.78it/s, episode_len=4][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.78it/s, episode_len=7][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.78it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.08it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.08it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.08it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.08it/s, episode_len=6] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.08it/s, episode_len=8][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 32.40it/s, episode_len=8][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 32.40it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 32.40it/s, episode_len=14][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 32.40it/s, episode_len=19][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 32.40it/s, episode_len=12][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.19it/s, episode_len=12][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.19it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.19it/s, episode_len=2] [A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.19it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:01<00:01, 32.19it/s, episode_len=12][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 34.04it/s, episode_len=12][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 34.04it/s, episode_len=14][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 34.04it/s, 
...[truncated]
================================================================================
Trial 26 | time = 2025-11-17T01:28:54.835636
Command: python -m src.train_rl_ppo --config config_trial_026.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.06691
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  3.859
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  4.825
  - reward_weights.physchem: 0.4328  ->  0.461
  - reward_weights.target_prob: 0.468  ->  0.2812
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.4468
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01276

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.06691, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=8][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.51it/s, episode_len=8][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.51it/s, episode_len=5][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.51it/s, episode_len=11][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.51it/s, episode_len=4] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.51it/s, episode_len=8][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.51it/s, episode_len=2][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.51it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 21.52it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 21.52it/s, episode_len=1] [A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 21.52it/s, episode_len=13][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 21.52it/s, episode_len=18][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 21.52it/s, episode_len=19][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 21.52it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 27.41it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 27.41it/s, episode_len=19][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 27.41it/s, episode_len=1] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 27.41it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 27.41it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.83it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.83it/s, episode_len=17][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.83it/s, episode_len=1] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.83it/s, episode_len=4][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.83it/s, episode_len=17][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.83it/s, episode_len=7] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.83it/s, episode_len=7][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 38.34it/s, episode_len=7][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 38.34it/s, episode_len=11][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 38.34it/s, episode_len=8] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 38.34it/s, episode_len=16][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 38.34it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 38.34it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 37.11it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 37.11it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 37.11it/s, episode_len=10][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 37.11it/s, episod
...[truncated]
================================================================================
Trial 27 | time = 2025-11-17T02:03:01.473698
Command: python -m src.train_rl_ppo --config config_trial_027.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.06463
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  2.467
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  3.899
  - reward_weights.physchem: 0.4328  ->  0.2705
  - reward_weights.target_prob: 0.468  ->  0.446
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1361
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.0315

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.06463, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=5] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=16][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=10][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=13][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.69it/s, episode_len=13][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.69it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.69it/s, episode_len=12][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.69it/s, episode_len=10][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.69it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.36it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.36it/s, episode_len=15][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.36it/s, episode_len=5] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.36it/s, episode_len=10][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.36it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.36it/s, episode_len=5] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 29.73it/s, episode_len=5][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 29.73it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 29.73it/s, episode_len=10][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 29.73it/s, episode_len=4] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 29.73it/s, episode_len=4][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 29.73it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 34.61it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 34.61it/s, episode_len=19][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 34.61it/s, episode_len=16][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 34.61it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 34.61it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 32.38it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 32.38it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 32.38it/s, episode_len=10][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 32.38it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 32.38it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 31.72it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 31.72it/s
...[truncated]
================================================================================
Trial 28 | time = 2025-11-17T02:39:07.727669
Command: python -m src.train_rl_ppo --config config_trial_028.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.01014
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  2.604
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  8.428
  - reward_weights.physchem: 0.4328  ->  0.2439
  - reward_weights.target_prob: 0.468  ->  0.2864
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.09499
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.02267

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.01014, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=5] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=7][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=2][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=2][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=4] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=16][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 22.91it/s, episode_len=16][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 22.91it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 22.91it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 22.91it/s, episode_len=3] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 22.91it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 25.93it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 25.93it/s, episode_len=1] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 25.93it/s, episode_len=13][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 25.93it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 25.93it/s, episode_len=15][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 25.93it/s, episode_len=6] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 31.82it/s, episode_len=6][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 31.82it/s, episode_len=1][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 31.82it/s, episode_len=3][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 31.82it/s, episode_len=5][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 31.82it/s, episode_len=9][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 31.82it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 31.82it/s, episode_len=3] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 31.82it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:00, 40.09it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:00, 40.09it/s, episode_len=5] [A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:00, 40.09it/s, episode_len=11][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:00, 40.09it/s, episode_len=13][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:00, 40.09it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:00, 40.09it/s, episode_len=4] [A

Collecting trajectories:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/64 [00:00<00:00, 42.11it/s, episode_len=4][A

Collecting trajectories:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/64 [00:00<00:00, 42.11it/s, episode
...[truncated]
================================================================================
Trial 29 | time = 2025-11-17T03:12:58.120935
Command: python -m src.train_rl_ppo --config config_trial_029.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.04959
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  3.424
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  3.565
  - reward_weights.physchem: 0.4328  ->  0.3077
  - reward_weights.target_prob: 0.468  ->  0.3127
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2272
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03911

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.04959, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.16it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.16it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.16it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.16it/s, episode_len=17][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.10it/s, episode_len=17][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.10it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.10it/s, episode_len=5] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.10it/s, episode_len=4][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.10it/s, episode_len=4][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.10it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 22.50it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 22.50it/s, episode_len=4] [A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 22.50it/s, episode_len=9][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 22.50it/s, episode_len=19][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 22.50it/s, episode_len=5] [A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 22.50it/s, episode_len=17][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 30.17it/s, episode_len=17][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 30.17it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 30.17it/s, episode_len=7] [A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 30.17it/s, episode_len=8][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 30.17it/s, episode_len=15][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 30.17it/s, episode_len=11][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 34.08it/s, episode_len=11][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 34.08it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 34.08it/s, episode_len=4] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 34.08it/s, episode_len=14][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 34.08it/s, episode_len=1] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 34.08it/s, episode_len=2][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 34.08it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 39.36it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 39.36it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 39.36it/s, episode_len=13][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 39.36it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 39.36it/s, episode_len=9] [A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:00, 39.36it/s, episod
...[truncated]
================================================================================
Trial 30 | time = 2025-11-17T03:47:36.489762
Command: python -m src.train_rl_ppo --config config_trial_030.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.01168
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  2.02
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  7.867
  - reward_weights.physchem: 0.4328  ->  0.2486
  - reward_weights.target_prob: 0.468  ->  0.3449
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1231
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03386

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.01168, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.15it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.15it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.15it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.15it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.87it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.87it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.87it/s, episode_len=3] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.87it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.87it/s, episode_len=1] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.87it/s, episode_len=17][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.06it/s, episode_len=17][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.06it/s, episode_len=10][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.06it/s, episode_len=8] [A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.06it/s, episode_len=18][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.06it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.63it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.63it/s, episode_len=4] [A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.63it/s, episode_len=14][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.63it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.63it/s, episode_len=1] [A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.63it/s, episode_len=2][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.63it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 33.40it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 33.40it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 33.40it/s, episode_len=4] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 33.40it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 33.40it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.88it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.88it/s, episode_len=9] [A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.88it/s, episode_len=14][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:01<00:01, 32.88it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:01<00:01, 32.88it/s, episode_len=19][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 32.73it/s, episode_len=19][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 32.73it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 32.73it/s, e
...[truncated]
================================================================================
Trial 31 | time = 2025-11-17T04:24:27.784786
Command: python -m src.train_rl_ppo --config config_trial_031.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.01254
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  2.617
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  8.448
  - reward_weights.physchem: 0.4328  ->  0.4091
  - reward_weights.target_prob: 0.468  ->  0.4206
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.4033
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01654

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.01254, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=3][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.68it/s, episode_len=3][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.68it/s, episode_len=6][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.68it/s, episode_len=7][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.68it/s, episode_len=4][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.68it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.68it/s, episode_len=3] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:02, 19.47it/s, episode_len=3][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:02, 19.47it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:02, 19.47it/s, episode_len=11][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:02, 19.47it/s, episode_len=11][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:02, 19.47it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.24it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.24it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.24it/s, episode_len=1] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.24it/s, episode_len=2][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.24it/s, episode_len=1][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.24it/s, episode_len=5][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.24it/s, episode_len=4][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.24it/s, episode_len=1][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.24it/s, episode_len=1][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.24it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 42.82it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 42.82it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 42.82it/s, episode_len=3] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 42.82it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 42.82it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 42.82it/s, episode_len=17][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 42.82it/s, episode_len=3] [A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 40.18it/s, episode_len=3][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 40.18it/s, episode_len=15][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 40.18it/s, episode_len=7] [A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 40.18it/s, episode_len=2][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 40.18it/s, episode_len=11][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 40.18it/s, episode_len=20][A

Collecting trajectories:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 30/64 [00:00<00:00, 41.66it/s, episode_le
...[truncated]
================================================================================
Trial 32 | time = 2025-11-17T04:58:44.208722
Command: python -m src.train_rl_ppo --config config_trial_032.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.0436
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  2.817
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  9.552
  - reward_weights.physchem: 0.4328  ->  0.4309
  - reward_weights.target_prob: 0.468  ->  0.2653
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.4158
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.04987

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.0436, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=6] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=8][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=9][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=2][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=14][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=5] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=18][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 22.50it/s, episode_len=18][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 22.50it/s, episode_len=5] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 22.50it/s, episode_len=1][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 22.50it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 22.50it/s, episode_len=6] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 22.50it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 30.32it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 30.32it/s, episode_len=1] [A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 30.32it/s, episode_len=19][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 30.32it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 30.32it/s, episode_len=18][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 30.32it/s, episode_len=3] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 33.96it/s, episode_len=3][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 33.96it/s, episode_len=15][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 33.96it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 33.96it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 33.96it/s, episode_len=13][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 33.96it/s, episode_len=12][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.87it/s, episode_len=12][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.87it/s, episode_len=10][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.87it/s, episode_len=2] [A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.87it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.87it/s, episode_len=17][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.87it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:00<00:01, 33.97it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:00<00:01, 33.97it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 33.97it/s, episo
...[truncated]
================================================================================
Trial 33 | time = 2025-11-17T05:32:39.131572
Command: python -m src.train_rl_ppo --config config_trial_033.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.0417
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  3.012
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  9.203
  - reward_weights.physchem: 0.4328  ->  0.3751
  - reward_weights.target_prob: 0.468  ->  0.2717
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3282
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03409

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.0417, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=17][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=3] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=15][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.85it/s, episode_len=15][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.85it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.85it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.85it/s, episode_len=19][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 17.72it/s, episode_len=19][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 17.72it/s, episode_len=13][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 17.72it/s, episode_len=14][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 17.72it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 17.72it/s, episode_len=15][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.49it/s, episode_len=15][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.49it/s, episode_len=4] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.49it/s, episode_len=7][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.49it/s, episode_len=7][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.49it/s, episode_len=4][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.49it/s, episode_len=12][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.49it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 32.29it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 32.29it/s, episode_len=12][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 32.29it/s, episode_len=9] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 32.29it/s, episode_len=10][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 32.29it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 34.09it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 34.09it/s, episode_len=17][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 34.09it/s, episode_len=7] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 34.09it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 34.09it/s, episode_len=18][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 33.62it/s, episode_len=18][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 33.62it/s, episode_len=2] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 33.62it/s, episode_len=6][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 33.62it/s, epis
...[truncated]
================================================================================
Trial 34 | time = 2025-11-17T06:06:44.614371
Command: python -m src.train_rl_ppo --config config_trial_034.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.03524
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  3.879
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  9.086
  - reward_weights.physchem: 0.4328  ->  0.3469
  - reward_weights.target_prob: 0.468  ->  0.3966
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2917
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01392

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.03524, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=5][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.66it/s, episode_len=5][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.66it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.66it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.66it/s, episode_len=19][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.49it/s, episode_len=19][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.49it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.49it/s, episode_len=13][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.49it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 16.92it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 16.92it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 16.92it/s, episode_len=16][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 16.92it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 20.36it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 20.36it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 20.36it/s, episode_len=3] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 20.36it/s, episode_len=12][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 20.36it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 25.69it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 25.69it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 25.69it/s, episode_len=2] [A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 25.69it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 25.69it/s, episode_len=12][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.55it/s, episode_len=12][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.55it/s, episode_len=2] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.55it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.55it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.55it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 30.91it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 30.91it/s, episode_len=6] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 30.91it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 30.91it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 30.91it/s, episode_len=19][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 31.29it/s, episode_len=19][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 31.29it/s, episode_len=5] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 31.29it/s,
...[truncated]
================================================================================
Trial 35 | time = 2025-11-17T06:40:25.768557
Command: python -m src.train_rl_ppo --config config_trial_035.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.08908
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  2.124
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  5.933
  - reward_weights.physchem: 0.4328  ->  0.4921
  - reward_weights.target_prob: 0.468  ->  0.2728
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3572
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.04963

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.08908, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.15it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.15it/s, episode_len=6] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.15it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.15it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.15it/s, episode_len=13][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.87it/s, episode_len=13][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.87it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.87it/s, episode_len=6] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.87it/s, episode_len=1][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.87it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.87it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.54it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.54it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.54it/s, episode_len=2] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.54it/s, episode_len=8][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.54it/s, episode_len=1][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.54it/s, episode_len=5][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.54it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 32.18it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 32.18it/s, episode_len=4] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 32.18it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 32.18it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 32.18it/s, episode_len=3] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 32.18it/s, episode_len=3][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 32.18it/s, episode_len=17][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 36.83it/s, episode_len=17][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 36.83it/s, episode_len=9] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 36.83it/s, episode_len=6][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 36.83it/s, episode_len=6][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 36.83it/s, episode_len=14][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 36.83it/s, episode_len=7] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 36.83it/s, episode_len=1][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 36.83it/s, episode_len=2][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 36.83it/s, episode_len=3][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 36.83it/s, episod
...[truncated]
================================================================================
Trial 36 | time = 2025-11-17T07:14:13.991003
Command: python -m src.train_rl_ppo --config config_trial_036.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.04176
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  3.947
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  5.787
  - reward_weights.physchem: 0.4328  ->  0.2004
  - reward_weights.target_prob: 0.468  ->  0.28
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.341
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.0101

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.04176, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=8][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.29it/s, episode_len=8][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.29it/s, episode_len=3][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.29it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.29it/s, episode_len=1] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.29it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.88it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.88it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.88it/s, episode_len=9] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.88it/s, episode_len=4][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.88it/s, episode_len=16][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.88it/s, episode_len=3] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.40it/s, episode_len=3][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.40it/s, episode_len=9][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.40it/s, episode_len=11][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.40it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.40it/s, episode_len=6] [A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 29.45it/s, episode_len=6][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 29.45it/s, episode_len=16][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 29.45it/s, episode_len=19][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 29.45it/s, episode_len=17][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 29.45it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.07it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.07it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.07it/s, episode_len=11][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.07it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.07it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 28.95it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 28.95it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 28.95it/s, episode_len=2] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 28.95it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 28.95it/s, episode_len=4] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 28.95it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 32.13it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 32.13it/s, episode_len=5] [A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 32.13it/s, epi
...[truncated]
================================================================================
Trial 37 | time = 2025-11-17T07:48:10.232456
Command: python -m src.train_rl_ppo --config config_trial_037.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.04767
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  2.132
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  7.162
  - reward_weights.physchem: 0.4328  ->  0.2985
  - reward_weights.target_prob: 0.468  ->  0.4049
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2458
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.02393

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.04767, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=5][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.69it/s, episode_len=5][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.69it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.69it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.69it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.75it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.75it/s, episode_len=18][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.75it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.75it/s, episode_len=6] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.75it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 18.98it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 18.98it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 18.98it/s, episode_len=11][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 18.98it/s, episode_len=14][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 18.98it/s, episode_len=4] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 18.98it/s, episode_len=4][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 27.58it/s, episode_len=4][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 27.58it/s, episode_len=17][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 27.58it/s, episode_len=3] [A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 27.58it/s, episode_len=15][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 27.58it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 30.45it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 30.45it/s, episode_len=6] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 30.45it/s, episode_len=19][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 30.45it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 30.45it/s, episode_len=9] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 32.61it/s, episode_len=9][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 32.61it/s, episode_len=14][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 32.61it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 32.61it/s, episode_len=3] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 32.61it/s, episode_len=10][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 32.61it/s, episode_len=17][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:01, 34.91it/s, episode_len=17][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:01, 34.91it/s, episode_len=10][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 34.91it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 34.91it/s, episo
...[truncated]
================================================================================
Trial 38 | time = 2025-11-17T08:23:51.303163
Command: python -m src.train_rl_ppo --config config_trial_038.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.05494
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  2.781
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  6.76
  - reward_weights.physchem: 0.4328  ->  0.364
  - reward_weights.target_prob: 0.468  ->  0.3146
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.4974
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.04242

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.05494, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=17][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=17][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.98it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.98it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.98it/s, episode_len=2] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.98it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.98it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.45it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.45it/s, episode_len=19][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.45it/s, episode_len=11][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.45it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 21.74it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 21.74it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 21.74it/s, episode_len=5] [A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 21.74it/s, episode_len=8][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 21.74it/s, episode_len=13][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 21.74it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 27.60it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 27.60it/s, episode_len=16][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 27.60it/s, episode_len=13][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 27.60it/s, episode_len=2] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 27.60it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 30.98it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 30.98it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 30.98it/s, episode_len=7] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 30.98it/s, episode_len=5][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 30.98it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 33.43it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 33.43it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 33.43it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 33.43it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 33.43it/s, episode_len=16][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 30.89it/s, 
...[truncated]
================================================================================
Trial 39 | time = 2025-11-17T08:57:51.889587
Command: python -m src.train_rl_ppo --config config_trial_039.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.06971
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  3.006
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  4.722
  - reward_weights.physchem: 0.4328  ->  0.3411
  - reward_weights.target_prob: 0.468  ->  0.3238
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1093
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.00912

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.06971, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=13][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.32it/s, episode_len=13][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.32it/s, episode_len=1] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.32it/s, episode_len=11][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.32it/s, episode_len=5] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.32it/s, episode_len=10][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.32it/s, episode_len=12][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.32it/s, episode_len=7] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.32it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 22.48it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 22.48it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 22.48it/s, episode_len=1] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 22.48it/s, episode_len=4][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 22.48it/s, episode_len=12][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 22.48it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 29.49it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 29.49it/s, episode_len=1] [A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 29.49it/s, episode_len=3][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 29.49it/s, episode_len=2][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 29.49it/s, episode_len=6][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 29.49it/s, episode_len=15][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 29.49it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 29.49it/s, episode_len=14][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 38.75it/s, episode_len=14][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 38.75it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 38.75it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 38.75it/s, episode_len=5] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 38.75it/s, episode_len=13][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 38.75it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 36.51it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 36.51it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 36.51it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 36.51it/s, episode_len=4] [A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 36.51it/s, episode_len=16][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 36.51it/s, episode_len=20][A

Collecting trajectories:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 30/64 [00:00<00:00, 34.88it/s, epi
...[truncated]
================================================================================
Trial 40 | time = 2025-11-17T09:33:55.423993
Command: python -m src.train_rl_ppo --config config_trial_040.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.02176
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  2.681
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  3.085
  - reward_weights.physchem: 0.4328  ->  0.3692
  - reward_weights.target_prob: 0.468  ->  0.358
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.179
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.04797

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.02176, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=2][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.81it/s, episode_len=2][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.81it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.81it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.81it/s, episode_len=2] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.81it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.83it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.83it/s, episode_len=8] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.83it/s, episode_len=6][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.83it/s, episode_len=17][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.83it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.83it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.37it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.37it/s, episode_len=3] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.37it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.37it/s, episode_len=9] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.37it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.37it/s, episode_len=1] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.95it/s, episode_len=1][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.95it/s, episode_len=12][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.95it/s, episode_len=19][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.95it/s, episode_len=5] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.95it/s, episode_len=14][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.95it/s, episode_len=3] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 36.45it/s, episode_len=3][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 36.45it/s, episode_len=7][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 36.45it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 36.45it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 36.45it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 36.45it/s, episode_len=3] [A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 37.11it/s, episode_len=3][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 37.11it/s, episode_len=13][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 37.11it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 37.11it/s, episode_len=7] [A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 37.11it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 37.11it/s, epi
...[truncated]
================================================================================
Trial 41 | time = 2025-11-17T10:11:14.022918
Command: python -m src.train_rl_ppo --config config_trial_041.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.04813
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  2.145
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  7.752
  - reward_weights.physchem: 0.4328  ->  0.4175
  - reward_weights.target_prob: 0.468  ->  0.4721
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3429
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.0412

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.04813, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=12][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.28it/s, episode_len=12][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.28it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.28it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.28it/s, episode_len=14][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.55it/s, episode_len=14][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.55it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.55it/s, episode_len=10][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.55it/s, episode_len=8] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.55it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.33it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.33it/s, episode_len=4] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.33it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.33it/s, episode_len=11][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.33it/s, episode_len=9] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.33it/s, episode_len=13][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 27.19it/s, episode_len=13][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 27.19it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 27.19it/s, episode_len=18][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 27.19it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 27.19it/s, episode_len=3] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.19it/s, episode_len=3][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.19it/s, episode_len=15][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.19it/s, episode_len=6] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.19it/s, episode_len=3][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.19it/s, episode_len=1][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.19it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.19it/s, episode_len=16][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 35.75it/s, episode_len=16][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 35.75it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 35.75it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 35.75it/s, episode_len=14][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:01<00:01, 35.75it/s, episode_len=7] [A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 35.00it/s, episode_len=7][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 35.00it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 35.00it/s, epis
...[truncated]
================================================================================
Trial 42 | time = 2025-11-17T10:49:25.279526
Command: python -m src.train_rl_ppo --config config_trial_042.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.04072
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  3.569
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  8.016
  - reward_weights.physchem: 0.4328  ->  0.4137
  - reward_weights.target_prob: 0.468  ->  0.2763
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3882
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.04575

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.04072, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.98it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.98it/s, episode_len=5] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.98it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.98it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.98it/s, episode_len=17][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.45it/s, episode_len=17][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.45it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.45it/s, episode_len=14][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.45it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 21.28it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 21.28it/s, episode_len=6] [A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 21.28it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 21.28it/s, episode_len=14][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 21.28it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 25.34it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 25.34it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 25.34it/s, episode_len=1] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 25.34it/s, episode_len=5][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 25.34it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 25.34it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 29.73it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 29.73it/s, episode_len=10][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 29.73it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:01<00:01, 29.73it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:01<00:01, 29.73it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 29.52it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 29.52it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 29.52it/s, episode_len=12][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 29.52it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 29.52it/s, episode_len=19][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 29.21it/s,
...[truncated]
================================================================================
Trial 43 | time = 2025-11-17T11:27:21.708161
Command: python -m src.train_rl_ppo --config config_trial_043.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.01276
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  3.129
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  4.569
  - reward_weights.physchem: 0.4328  ->  0.3195
  - reward_weights.target_prob: 0.468  ->  0.3923
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.4664
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.04102

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.01276, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=2][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.77it/s, episode_len=2][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.77it/s, episode_len=1][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.77it/s, episode_len=13][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.77it/s, episode_len=7] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.77it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 16.35it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 16.35it/s, episode_len=12][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 16.35it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 16.35it/s, episode_len=14][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 16.35it/s, episode_len=5] [A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 24.11it/s, episode_len=5][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 24.11it/s, episode_len=11][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 24.11it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 24.11it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 24.11it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.79it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.79it/s, episode_len=6] [A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.79it/s, episode_len=19][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.79it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.79it/s, episode_len=10][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.11it/s, episode_len=10][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.11it/s, episode_len=11][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.11it/s, episode_len=11][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.11it/s, episode_len=4] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.11it/s, episode_len=5][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.11it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 34.94it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 34.94it/s, episode_len=3] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 34.94it/s, episode_len=9][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 34.94it/s, episode_len=15][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 34.94it/s, episode_len=3] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 34.94it/s, episode_len=13][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 34.94it/s, episode_len=10][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:00<00:00, 41.37it/s, episode_len=10][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:00<00:00, 41.37it/s, episod
...[truncated]
================================================================================
Trial 44 | time = 2025-11-17T12:04:49.923675
Command: python -m src.train_rl_ppo --config config_trial_044.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.07275
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  2.869
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  5.649
  - reward_weights.physchem: 0.4328  ->  0.4551
  - reward_weights.target_prob: 0.468  ->  0.3526
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3913
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.02005

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.07275, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=14][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=14][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=10][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=1] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.49it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.49it/s, episode_len=6] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.49it/s, episode_len=12][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.49it/s, episode_len=8] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.49it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.49it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.15it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.15it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.15it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.15it/s, episode_len=3] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.15it/s, episode_len=3][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.15it/s, episode_len=14][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 29.22it/s, episode_len=14][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 29.22it/s, episode_len=12][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 29.22it/s, episode_len=15][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 29.22it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 29.22it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 29.47it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 29.47it/s, episode_len=3] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 29.47it/s, episode_len=3][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 29.47it/s, episode_len=18][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 29.47it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 29.47it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 32.47it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 32.47it/s, episode_len=4] [A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 32.47it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 32.47it/s, episode_len=5] [A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 32.47it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 32.47it/s, episode_len=7] [A

Collecting trajectories:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/64 [00:01<00:00, 36.03it/s
...[truncated]
================================================================================
Trial 45 | time = 2025-11-17T12:43:43.109491
Command: python -m src.train_rl_ppo --config config_trial_045.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.08686
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  3.879
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  6.008
  - reward_weights.physchem: 0.4328  ->  0.3927
  - reward_weights.target_prob: 0.468  ->  0.3694
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2363
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03185

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.08686, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=19][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=19][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=1] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=11][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=4] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=14][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 18.82it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 18.82it/s, episode_len=14][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 18.82it/s, episode_len=7] [A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 18.82it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 18.82it/s, episode_len=8] [A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 18.82it/s, episode_len=2][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 18.82it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 27.71it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 27.71it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 27.71it/s, episode_len=8] [A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 27.71it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 27.71it/s, episode_len=16][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.17it/s, episode_len=16][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.17it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.17it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.17it/s, episode_len=7] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 29.17it/s, episode_len=11][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.03it/s, episode_len=11][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.03it/s, episode_len=12][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.03it/s, episode_len=9] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.03it/s, episode_len=12][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.03it/s, episode_len=6] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.03it/s, episode_len=4][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.03it/s, episode_len=14][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 37.53it/s, episode_len=14][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 37.53it/s, episode_len=1] [A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 37.53it/s, episode_len=1][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 37.53it/s, ep
...[truncated]
================================================================================
Trial 46 | time = 2025-11-17T13:22:03.157033
Command: python -m src.train_rl_ppo --config config_trial_046.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.0628
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  2.814
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  5.872
  - reward_weights.physchem: 0.4328  ->  0.2771
  - reward_weights.target_prob: 0.468  ->  0.2862
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3383
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01873

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.0628, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=17][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=4] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=16][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.43it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.43it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.43it/s, episode_len=7] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.43it/s, episode_len=5][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.43it/s, episode_len=6][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.43it/s, episode_len=13][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.43it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.63it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.63it/s, episode_len=9] [A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.63it/s, episode_len=11][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.63it/s, episode_len=1] [A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.63it/s, episode_len=3][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.63it/s, episode_len=1][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.63it/s, episode_len=1][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.63it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.63it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 38.24it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 38.24it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 38.24it/s, episode_len=11][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 38.24it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 38.24it/s, episode_len=13][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 38.24it/s, episode_len=5] [A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 38.25it/s, episode_len=5][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 38.25it/s, episode_len=9][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 38.25it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 38.25it/s, episode_len=3] [A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 38.25it/s, episode_len=19][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 38.25it/s, episode_len=20][A

Collecting trajectories:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/64 [00:00<00:00, 37.93it/s, episode_len=20][A

Collecting trajectories:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/64 [00:00<00:00, 37.93it/s, episo
...[truncated]
================================================================================
Trial 47 | time = 2025-11-17T14:00:03.266081
Command: python -m src.train_rl_ppo --config config_trial_047.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.04451
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  2.51
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  4.582
  - reward_weights.physchem: 0.4328  ->  0.2432
  - reward_weights.target_prob: 0.468  ->  0.3096
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2561
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.02621

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.04451, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=12][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.37it/s, episode_len=12][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.37it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.37it/s, episode_len=1] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.37it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.37it/s, episode_len=9] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.37it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.73it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.73it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.73it/s, episode_len=9] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.73it/s, episode_len=8][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.73it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.08it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.08it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.08it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.08it/s, episode_len=6] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.08it/s, episode_len=6][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.08it/s, episode_len=1][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.90it/s, episode_len=1][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.90it/s, episode_len=13][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.90it/s, episode_len=4] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.90it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.90it/s, episode_len=11][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.90it/s, episode_len=8] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 35.90it/s, episode_len=8][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 35.90it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 35.90it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 35.90it/s, episode_len=4] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 35.90it/s, episode_len=3][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 35.90it/s, episode_len=2][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 35.90it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 39.58it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 39.58it/s, episode_len=2] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 39.58it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 39.58it/s, episode_len=13][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 39.58it/s, epi
...[truncated]
================================================================================
Trial 48 | time = 2025-11-17T14:37:49.131817
Command: python -m src.train_rl_ppo --config config_trial_048.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.05181
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  2.989
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  6.272
  - reward_weights.physchem: 0.4328  ->  0.2093
  - reward_weights.target_prob: 0.468  ->  0.271
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3684
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01099

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.05181, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=16][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.29it/s, episode_len=16][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.29it/s, episode_len=1] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.29it/s, episode_len=3][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.29it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.29it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.29it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.00it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.00it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.00it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.00it/s, episode_len=1] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.00it/s, episode_len=19][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.87it/s, episode_len=19][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.87it/s, episode_len=11][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.87it/s, episode_len=7] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.87it/s, episode_len=12][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.87it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.87it/s, episode_len=2] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.60it/s, episode_len=2][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.60it/s, episode_len=9][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.60it/s, episode_len=13][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.60it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.60it/s, episode_len=11][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 33.06it/s, episode_len=11][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 33.06it/s, episode_len=4] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 33.06it/s, episode_len=6][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 33.06it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 33.06it/s, episode_len=9] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 33.06it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 36.67it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 36.67it/s, episode_len=5] [A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 36.67it/s, episode_len=11][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 36.67it/s, episode_len=6] [A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 36.67it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 36.67it/s, episode_len=6] [A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 36.67it/s, 
...[truncated]
================================================================================
Trial 49 | time = 2025-11-17T15:16:59.547794
Command: python -m src.train_rl_ppo --config config_trial_049.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.08161
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  2.788
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  8.815
  - reward_weights.physchem: 0.4328  ->  0.2609
  - reward_weights.target_prob: 0.468  ->  0.3842
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2203
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03511

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.08161, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=11][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.40it/s, episode_len=11][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.40it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.40it/s, episode_len=8] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.40it/s, episode_len=2][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.40it/s, episode_len=5][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.40it/s, episode_len=19][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 18.51it/s, episode_len=19][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 18.51it/s, episode_len=6] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 18.51it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 18.51it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 18.51it/s, episode_len=11][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.67it/s, episode_len=11][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.67it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.67it/s, episode_len=8] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.67it/s, episode_len=9][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.67it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 28.51it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 28.51it/s, episode_len=5] [A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 28.51it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 28.51it/s, episode_len=10][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 28.51it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 31.50it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 31.50it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 31.50it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 31.50it/s, episode_len=1] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 31.50it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 32.54it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 32.54it/s, episode_len=1] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 32.54it/s, episode_len=3][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 32.54it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 32.54it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 32.54it/s, episode_len=11][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 37.07it/s, episode_len=11][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 37.07it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 37.07it/s, 
...[truncated]
================================================================================
Trial 50 | time = 2025-11-17T15:56:53.211492
Command: python -m src.train_rl_ppo --config config_trial_050.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.0211
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  2.618
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  7.637
  - reward_weights.physchem: 0.4328  ->  0.435
  - reward_weights.target_prob: 0.468  ->  0.3893
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.4997
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.006276

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.0211, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=1][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.84it/s, episode_len=1][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.84it/s, episode_len=3][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.84it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.84it/s, episode_len=16][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:04, 12.92it/s, episode_len=16][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:04, 12.92it/s, episode_len=9] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:04, 12.92it/s, episode_len=10][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:04, 12.92it/s, episode_len=13][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:04, 12.92it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 21.67it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 21.67it/s, episode_len=9] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 21.67it/s, episode_len=9][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 21.67it/s, episode_len=14][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 21.67it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 27.20it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 27.20it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 27.20it/s, episode_len=18][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 27.20it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 27.20it/s, episode_len=10][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 28.24it/s, episode_len=10][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 28.24it/s, episode_len=12][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 28.24it/s, episode_len=19][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 28.24it/s, episode_len=3] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 28.24it/s, episode_len=6][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 28.24it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 32.83it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 32.83it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 32.83it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 32.83it/s, episode_len=6] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 32.83it/s, episode_len=4][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 32.83it/s, episode_len=1][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 32.83it/s, episode_len=14][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 37.56it/s, episode_len=14][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 37.56it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:00, 37.56it/s, episode
...[truncated]
================================================================================
Trial 51 | time = 2025-11-17T16:35:40.479780
Command: python -m src.train_rl_ppo --config config_trial_051.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.0287
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  3.318
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  6.041
  - reward_weights.physchem: 0.4328  ->  0.3465
  - reward_weights.target_prob: 0.468  ->  0.4312
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2053
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.02154

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.0287, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=3][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.75it/s, episode_len=3][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.75it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.75it/s, episode_len=4] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.75it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:04, 12.74it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:04, 12.74it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:04, 12.74it/s, episode_len=18][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:04, 12.74it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 17.69it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 17.69it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 17.69it/s, episode_len=17][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 17.69it/s, episode_len=10][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 17.69it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 22.35it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 22.35it/s, episode_len=8] [A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 22.35it/s, episode_len=18][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 22.35it/s, episode_len=7] [A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 22.35it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 27.26it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 27.26it/s, episode_len=7] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 27.26it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 27.26it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 27.26it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.51it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.51it/s, episode_len=1] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.51it/s, episode_len=16][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.51it/s, episode_len=12][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.51it/s, episode_len=1] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.51it/s, episode_len=17][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.51it/s, episode_len=17][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 34.76it/s, episode_len=17][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 34.76it/s, episode_len=9] [A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 34.76it/s, episode_len=19][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 34.76it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 34.76it/s, e
...[truncated]
================================================================================
Trial 52 | time = 2025-11-17T17:13:29.963989
Command: python -m src.train_rl_ppo --config config_trial_052.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.0553
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  3.445
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  3.571
  - reward_weights.physchem: 0.4328  ->  0.4251
  - reward_weights.target_prob: 0.468  ->  0.4223
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3544
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01643

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.0553, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=3] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=4][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.93it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.93it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.93it/s, episode_len=2] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.93it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.93it/s, episode_len=7] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.93it/s, episode_len=14][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 23.99it/s, episode_len=14][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 23.99it/s, episode_len=2] [A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 23.99it/s, episode_len=16][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 23.99it/s, episode_len=16][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 23.99it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 27.66it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 27.66it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 27.66it/s, episode_len=2] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 27.66it/s, episode_len=2][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 27.66it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 27.66it/s, episode_len=10][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 33.18it/s, episode_len=10][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 33.18it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 33.18it/s, episode_len=7] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 33.18it/s, episode_len=9][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 33.18it/s, episode_len=18][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 34.42it/s, episode_len=18][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 34.42it/s, episode_len=11][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 34.42it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 34.42it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 34.42it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 32.34it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 32.34it/s, e
...[truncated]
================================================================================
Trial 53 | time = 2025-11-17T17:53:31.993044
Command: python -m src.train_rl_ppo --config config_trial_053.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.01751
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  2.202
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  4.503
  - reward_weights.physchem: 0.4328  ->  0.2964
  - reward_weights.target_prob: 0.468  ->  0.3256
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2968
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01602

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.01751, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=8] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=6] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.44it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.44it/s, episode_len=16][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.44it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.44it/s, episode_len=7] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.44it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.77it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.77it/s, episode_len=1] [A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.77it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.77it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.77it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:02, 24.90it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:02, 24.90it/s, episode_len=14][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:02, 24.90it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:02, 24.90it/s, episode_len=13][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:02, 24.90it/s, episode_len=12][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 27.77it/s, episode_len=12][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 27.77it/s, episode_len=9] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 27.77it/s, episode_len=17][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 27.77it/s, episode_len=1] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 27.77it/s, episode_len=6][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 27.77it/s, episode_len=9][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 27.77it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 34.50it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 34.50it/s, episode_len=11][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 34.50it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 34.50it/s, episode_len=1] [A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 34.50it/s, episode_len=11][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:01<00:01, 34.50it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:00, 36.26it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:00, 36.26it/s, ep
...[truncated]
================================================================================
Trial 54 | time = 2025-11-17T18:33:48.919032
Command: python -m src.train_rl_ppo --config config_trial_054.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.082
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  2.818
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  3.5
  - reward_weights.physchem: 0.4328  ->  0.2074
  - reward_weights.target_prob: 0.468  ->  0.426
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.422
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.04477

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.082, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=16][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.34it/s, episode_len=16][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.34it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.34it/s, episode_len=14][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.34it/s, episode_len=1] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.34it/s, episode_len=9][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.34it/s, episode_len=10][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.84it/s, episode_len=10][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.84it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.84it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.84it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 23.18it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 23.18it/s, episode_len=7] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 23.18it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 23.18it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 23.18it/s, episode_len=11][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 27.17it/s, episode_len=11][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 27.17it/s, episode_len=5] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 27.17it/s, episode_len=9][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 27.17it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 27.17it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 30.52it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 30.52it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 30.52it/s, episode_len=13][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 30.52it/s, episode_len=16][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:01<00:01, 30.52it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 30.46it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 30.46it/s, episode_len=14][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 30.46it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 30.46it/s, episode_len=8] [A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 30.46it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 31.48it/s, epi
...[truncated]
================================================================================
Trial 55 | time = 2025-11-17T19:13:12.472400
Command: python -m src.train_rl_ppo --config config_trial_055.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.03134
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  2.573
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  8.377
  - reward_weights.physchem: 0.4328  ->  0.237
  - reward_weights.target_prob: 0.468  ->  0.3602
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.4525
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.04491

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.03134, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=4] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=9][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.36it/s, episode_len=9][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.36it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.36it/s, episode_len=14][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.36it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.65it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.65it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.65it/s, episode_len=13][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.65it/s, episode_len=2] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.65it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 24.21it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 24.21it/s, episode_len=9] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 24.21it/s, episode_len=7][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 24.21it/s, episode_len=10][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 24.21it/s, episode_len=4] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 24.21it/s, episode_len=12][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 24.21it/s, episode_len=3] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 24.21it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 34.48it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 34.48it/s, episode_len=15][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 34.48it/s, episode_len=14][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 34.48it/s, episode_len=8] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 34.48it/s, episode_len=15][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 35.61it/s, episode_len=15][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 35.61it/s, episode_len=4] [A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 35.61it/s, episode_len=7][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 35.61it/s, episode_len=2][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 35.61it/s, episode_len=2][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 35.61it/s, episode_len=17][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 35.61it/s, episode_len=17][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 35.61it/s, episo
...[truncated]
================================================================================
Trial 56 | time = 2025-11-17T19:52:40.023395
Command: python -m src.train_rl_ppo --config config_trial_056.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.08336
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  3.756
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  8.022
  - reward_weights.physchem: 0.4328  ->  0.3442
  - reward_weights.target_prob: 0.468  ->  0.2519
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3057
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03695

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.08336, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=1][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.82it/s, episode_len=1][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.82it/s, episode_len=7][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.82it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.82it/s, episode_len=3] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.82it/s, episode_len=7][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.82it/s, episode_len=8][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:02, 19.77it/s, episode_len=8][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:02, 19.77it/s, episode_len=6][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:02, 19.77it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:02, 19.77it/s, episode_len=12][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:02, 19.77it/s, episode_len=18][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.61it/s, episode_len=18][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.61it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.61it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.61it/s, episode_len=6] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.61it/s, episode_len=2][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.61it/s, episode_len=19][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.15it/s, episode_len=19][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.15it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.15it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.15it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 30.15it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.34it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.34it/s, episode_len=16][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.34it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.34it/s, episode_len=5] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.34it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 29.85it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 29.85it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 29.85it/s, episode_len=8] [A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 29.85it/s, episode_len=12][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 29.85it/s, episode_len=14][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:01, 31.98it/s, episode_len=14][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 31.98it/s, episode_len=14][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 31.98it/s, epis
...[truncated]
================================================================================
Trial 57 | time = 2025-11-17T20:32:17.512004
Command: python -m src.train_rl_ppo --config config_trial_057.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.05124
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  2.538
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  7.706
  - reward_weights.physchem: 0.4328  ->  0.4172
  - reward_weights.target_prob: 0.468  ->  0.4147
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2842
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.04466

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.05124, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.04it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.04it/s, episode_len=10][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.04it/s, episode_len=6] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.04it/s, episode_len=7][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.04it/s, episode_len=13][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.04it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.74it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.74it/s, episode_len=10][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.74it/s, episode_len=13][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.74it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.74it/s, episode_len=11][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.10it/s, episode_len=11][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.10it/s, episode_len=8] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.10it/s, episode_len=11][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.10it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.10it/s, episode_len=12][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 28.00it/s, episode_len=12][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 28.00it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 28.00it/s, episode_len=7] [A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 28.00it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 28.00it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.95it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.95it/s, episode_len=16][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.95it/s, episode_len=2] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.95it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.95it/s, episode_len=5] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.95it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.81it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.81it/s, episode_len=10][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.81it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.81it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.81it/s, episode_len=1] [A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:01<00:01, 32.81it/s, episode_len=9][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 35.69it/s, episode_len=9][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 35.69it/s,
...[truncated]
================================================================================
Trial 58 | time = 2025-11-17T21:11:57.708867
Command: python -m src.train_rl_ppo --config config_trial_058.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.01972
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  2.858
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  3.613
  - reward_weights.physchem: 0.4328  ->  0.3765
  - reward_weights.target_prob: 0.468  ->  0.2748
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1744
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03239

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.01972, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=2] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=8][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.91it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.91it/s, episode_len=4] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.91it/s, episode_len=14][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.91it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.91it/s, episode_len=9] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.91it/s, episode_len=8][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 24.95it/s, episode_len=8][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 24.95it/s, episode_len=7][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 24.95it/s, episode_len=2][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 24.95it/s, episode_len=2][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 24.95it/s, episode_len=2][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 24.95it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 24.95it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 34.44it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 34.44it/s, episode_len=1] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 34.44it/s, episode_len=18][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 34.44it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 34.44it/s, episode_len=16][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 34.44it/s, episode_len=10][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 35.92it/s, episode_len=10][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 35.92it/s, episode_len=18][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 35.92it/s, episode_len=12][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 35.92it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 35.92it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 35.92it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:01, 32.86it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:01, 32.86it/s, episode_len=4] [A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 32.86it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 32.86it/s, epis
...[truncated]
================================================================================
Trial 59 | time = 2025-11-17T21:51:46.433953
Command: python -m src.train_rl_ppo --config config_trial_059.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.05989
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  2.568
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  4.424
  - reward_weights.physchem: 0.4328  ->  0.2682
  - reward_weights.target_prob: 0.468  ->  0.4257
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1813
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03079

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.05989, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=12][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=12][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=8] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=6][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=7][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=5][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.28it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.28it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.28it/s, episode_len=12][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.28it/s, episode_len=16][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.07it/s, episode_len=16][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.07it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.07it/s, episode_len=4] [A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.07it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.54it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.54it/s, episode_len=10][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.54it/s, episode_len=8] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.54it/s, episode_len=19][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.54it/s, episode_len=13][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 25.93it/s, episode_len=13][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 25.93it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 25.93it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 25.93it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 23.99it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 23.99it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 23.99it/s, episode_len=11][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:01<00:01, 23.99it/s, episode_len=5] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:01<00:01, 23.99it/s, episode_len=12][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:01<00:01, 26.83it/s, episode_len=12][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:01<00:01, 26.83it/s, episode_len=15][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:01<00:01, 26.83it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:01<00:01, 26.83it/s, episode_len=11][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 26.74it/s, episode_len=11][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 26.74it/s, episode_len=3] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 26.74it/s, ep
...[truncated]
================================================================================
Trial 60 | time = 2025-11-17T22:30:24.538313
Command: python -m src.train_rl_ppo --config config_trial_060.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.03784
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  2.174
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  8.635
  - reward_weights.physchem: 0.4328  ->  0.2445
  - reward_weights.target_prob: 0.468  ->  0.3364
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.09796
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01778

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.03784, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=17][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.27it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.27it/s, episode_len=2] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.27it/s, episode_len=8][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.27it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.27it/s, episode_len=2] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.27it/s, episode_len=2][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.27it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.87it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.87it/s, episode_len=7] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.87it/s, episode_len=5][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.87it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.87it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.87it/s, episode_len=11][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 31.24it/s, episode_len=11][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 31.24it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 31.24it/s, episode_len=6] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 31.24it/s, episode_len=1][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 31.24it/s, episode_len=16][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 31.24it/s, episode_len=15][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 35.56it/s, episode_len=15][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 35.56it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 35.56it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 35.56it/s, episode_len=16][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 35.56it/s, episode_len=4] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 35.56it/s, episode_len=15][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 35.35it/s, episode_len=15][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 35.35it/s, episode_len=9] [A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 35.35it/s, episode_len=13][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 35.35it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 35.35it/s, episode_len=20][A

Collecting trajectories:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/64 [00:01<00:01, 34.82it/s, 
...[truncated]
================================================================================
Trial 61 | time = 2025-11-17T23:09:26.675897
Command: python -m src.train_rl_ppo --config config_trial_061.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.07527
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  3.297
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  4.708
  - reward_weights.physchem: 0.4328  ->  0.2118
  - reward_weights.target_prob: 0.468  ->  0.3954
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1565
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.0372

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.07527, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=13][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.70it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.70it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.70it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 10.70it/s, episode_len=11][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 15.38it/s, episode_len=11][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 15.38it/s, episode_len=13][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 15.38it/s, episode_len=13][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 15.38it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 18.99it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 18.99it/s, episode_len=9] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 18.99it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 18.99it/s, episode_len=10][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 18.99it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:02, 22.32it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:02, 22.32it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:02, 22.32it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:02, 22.32it/s, episode_len=12][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:02, 22.94it/s, episode_len=12][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:02, 22.94it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:01<00:02, 22.94it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:01<00:02, 22.94it/s, episode_len=11][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:01<00:01, 23.46it/s, episode_len=11][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:01<00:01, 23.46it/s, episode_len=7] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:01<00:01, 23.46it/s, episode_len=5][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:01<00:01, 23.46it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:01<00:01, 23.46it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 26.22it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 26.22it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 26.22it/s, episode_len=11][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 26.22it/s, episode_len=17][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 26.25it/s
...[truncated]
================================================================================
Trial 62 | time = 2025-11-17T23:49:18.819054
Command: python -m src.train_rl_ppo --config config_trial_062.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.0517
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  3.896
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  7.577
  - reward_weights.physchem: 0.4328  ->  0.4442
  - reward_weights.target_prob: 0.468  ->  0.423
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3421
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01241

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.0517, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=6][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.66it/s, episode_len=6][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.66it/s, episode_len=12][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.66it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.66it/s, episode_len=1] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.66it/s, episode_len=19][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 15.20it/s, episode_len=19][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 15.20it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 15.20it/s, episode_len=3] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 15.20it/s, episode_len=3][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 15.20it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 15.20it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.15it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.15it/s, episode_len=15][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.15it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.15it/s, episode_len=9] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.15it/s, episode_len=1][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.15it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 29.47it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 29.47it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 29.47it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 29.47it/s, episode_len=15][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 29.47it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.72it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.72it/s, episode_len=15][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.72it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.72it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 28.72it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 28.37it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 28.37it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:01<00:01, 28.37it/s, episode_len=15][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:01<00:01, 28.37it/s, episode_len=18][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:01<00:01, 28.37it/s, episode_len=9] [A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 29.76it/s, episode_len=9][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 29.76it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 29.76it/s, ep
...[truncated]
================================================================================
Trial 63 | time = 2025-11-18T00:28:06.106545
Command: python -m src.train_rl_ppo --config config_trial_063.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.03585
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  3.522
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  7.571
  - reward_weights.physchem: 0.4328  ->  0.3861
  - reward_weights.target_prob: 0.468  ->  0.3371
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.346
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01815

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.03585, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=17][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=6] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=8] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.79it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.79it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.79it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.79it/s, episode_len=19][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 18.96it/s, episode_len=19][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 18.96it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 18.96it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 18.96it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 21.11it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 21.11it/s, episode_len=19][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 21.11it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 21.11it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:02, 22.72it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:02, 22.72it/s, episode_len=11][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:02, 22.72it/s, episode_len=6] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:02, 22.72it/s, episode_len=18][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:02, 22.72it/s, episode_len=3] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:02, 22.72it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 29.27it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 29.27it/s, episode_len=2] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 29.27it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 29.27it/s, episode_len=10][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 29.27it/s, episode_len=11][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:01<00:01, 29.27it/s, episode_len=10][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 34.57it/s, episode_len=10][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 34.57it/s, episode_len=6] [A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 34.57it/s, episode_len=17][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 34.57it/s, episode_len=10][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 34.57it/s,
...[truncated]
================================================================================
Trial 64 | time = 2025-11-18T01:06:12.020359
Command: python -m src.train_rl_ppo --config config_trial_064.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.01484
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  3.25
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  8.79
  - reward_weights.physchem: 0.4328  ->  0.4708
  - reward_weights.target_prob: 0.468  ->  0.4217
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.342
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.008192

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.01484, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=9] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=2][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=2][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=7][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 19.83it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 19.83it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 19.83it/s, episode_len=8] [A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 19.83it/s, episode_len=14][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 19.83it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 24.27it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 24.27it/s, episode_len=12][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 24.27it/s, episode_len=5] [A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 24.27it/s, episode_len=2][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 24.27it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 24.27it/s, episode_len=14][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 31.28it/s, episode_len=14][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 31.28it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 31.28it/s, episode_len=2] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 31.28it/s, episode_len=14][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 31.28it/s, episode_len=15][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 31.28it/s, episode_len=1] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 36.49it/s, episode_len=1][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 36.49it/s, episode_len=12][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 36.49it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 36.49it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 36.49it/s, episode_len=1] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 36.49it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:01, 36.05it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:01, 36.05it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:01, 36.05it/s, episode_len=19][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:01, 36.05it/s, episode_len=7] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 36.05it/s, ep
...[truncated]
================================================================================
Trial 65 | time = 2025-11-18T01:45:26.542585
Command: python -m src.train_rl_ppo --config config_trial_065.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.03931
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  2.73
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  4.731
  - reward_weights.physchem: 0.4328  ->  0.2149
  - reward_weights.target_prob: 0.468  ->  0.4701
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2374
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.0311

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.03931, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=4][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.68it/s, episode_len=4][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.68it/s, episode_len=6][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.68it/s, episode_len=12][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.68it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.68it/s, episode_len=4] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.68it/s, episode_len=3][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 19.28it/s, episode_len=3][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 19.28it/s, episode_len=6][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 19.28it/s, episode_len=15][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 19.28it/s, episode_len=19][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 19.28it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.62it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.62it/s, episode_len=18][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.62it/s, episode_len=12][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.62it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.62it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 26.18it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 26.18it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 26.18it/s, episode_len=4] [A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 26.18it/s, episode_len=16][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 26.18it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.55it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.55it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.55it/s, episode_len=16][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.55it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.55it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 27.94it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 27.94it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 27.94it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 27.94it/s, episode_len=1] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 27.94it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 29.56it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 29.56it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 29.56it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 29.56it/s, ep
...[truncated]
================================================================================
Trial 66 | time = 2025-11-18T02:25:10.243352
Command: python -m src.train_rl_ppo --config config_trial_066.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.01579
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  2.29
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  5.868
  - reward_weights.physchem: 0.4328  ->  0.2942
  - reward_weights.target_prob: 0.468  ->  0.3632
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1998
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03217

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.01579, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.16it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.16it/s, episode_len=7] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.16it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.16it/s, episode_len=8] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.16it/s, episode_len=1][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.16it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.24it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.24it/s, episode_len=12][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.24it/s, episode_len=8] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.24it/s, episode_len=5][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.24it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.24it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.09it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.09it/s, episode_len=7] [A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.09it/s, episode_len=2][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.09it/s, episode_len=18][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.09it/s, episode_len=15][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.09it/s, episode_len=17][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.84it/s, episode_len=17][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.84it/s, episode_len=13][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.84it/s, episode_len=5] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.84it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 30.84it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 32.22it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 32.22it/s, episode_len=1] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 32.22it/s, episode_len=2][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 32.22it/s, episode_len=11][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 32.22it/s, episode_len=15][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 32.22it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 32.22it/s, episode_len=5] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 39.31it/s, episode_len=5][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 39.31it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 39.31it/s, episode_len=9] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 39.31it/s, episode_len=7][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 39.31it/s, epi
...[truncated]
================================================================================
Trial 67 | time = 2025-11-18T03:03:15.446731
Command: python -m src.train_rl_ppo --config config_trial_067.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.09788
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  2.875
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  4.319
  - reward_weights.physchem: 0.4328  ->  0.3219
  - reward_weights.target_prob: 0.468  ->  0.2644
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2404
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03611

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.09788, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=19][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.20it/s, episode_len=19][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.20it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.20it/s, episode_len=17][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.20it/s, episode_len=3] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.20it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.42it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.42it/s, episode_len=7] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.42it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.42it/s, episode_len=10][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.42it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.47it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.47it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.47it/s, episode_len=1] [A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.47it/s, episode_len=7][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.47it/s, episode_len=4][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.47it/s, episode_len=5][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.47it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 31.85it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 31.85it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 31.85it/s, episode_len=4] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 31.85it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 31.85it/s, episode_len=3] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 31.85it/s, episode_len=2][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 31.85it/s, episode_len=10][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 38.40it/s, episode_len=10][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 38.40it/s, episode_len=13][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 38.40it/s, episode_len=4] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 38.40it/s, episode_len=5][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 38.40it/s, episode_len=2][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 38.40it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 38.40it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 41.67it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 41.67it/s, episode_len=12][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 41.67it/s, episode_len=12][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 41.67it/s, episode_
...[truncated]
================================================================================
Trial 68 | time = 2025-11-18T03:44:16.777475
Command: python -m src.train_rl_ppo --config config_trial_068.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.01232
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  3.936
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  9.053
  - reward_weights.physchem: 0.4328  ->  0.4955
  - reward_weights.target_prob: 0.468  ->  0.4918
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1852
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.04971

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.01232, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=4][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.78it/s, episode_len=4][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.78it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.78it/s, episode_len=4] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.78it/s, episode_len=3][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.78it/s, episode_len=1][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.78it/s, episode_len=11][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.78it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 21.25it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 21.25it/s, episode_len=2] [A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 21.25it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 21.25it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 21.25it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.63it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.63it/s, episode_len=13][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.63it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.63it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.63it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 26.74it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 26.74it/s, episode_len=4] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 26.74it/s, episode_len=10][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 26.74it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 26.74it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 30.10it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 30.10it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 30.10it/s, episode_len=8] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 30.10it/s, episode_len=14][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 30.10it/s, episode_len=13][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.40it/s, episode_len=13][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.40it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.40it/s, episode_len=16][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.40it/s, episode_len=18][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 32.40it/s, episode_len=2] [A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:01, 33.87it/s, episode_len=2][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 33.87it/s, episode_len=19][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 33.87it/s, ep
...[truncated]
================================================================================
Trial 69 | time = 2025-11-18T04:24:00.555769
Command: python -m src.train_rl_ppo --config config_trial_069.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.05427
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  2.209
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  3.933
  - reward_weights.physchem: 0.4328  ->  0.3918
  - reward_weights.target_prob: 0.468  ->  0.4266
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.4044
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01223

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.05427, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=7][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.63it/s, episode_len=7][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.63it/s, episode_len=13][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.63it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.63it/s, episode_len=19][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.51it/s, episode_len=19][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.51it/s, episode_len=18][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.51it/s, episode_len=8] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.51it/s, episode_len=3][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.51it/s, episode_len=11][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.51it/s, episode_len=2] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.51it/s, episode_len=8][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.10it/s, episode_len=8][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.10it/s, episode_len=9][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.10it/s, episode_len=18][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.10it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.10it/s, episode_len=3] [A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.92it/s, episode_len=3][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.92it/s, episode_len=15][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.92it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.92it/s, episode_len=3] [A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.92it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.40it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.40it/s, episode_len=19][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.40it/s, episode_len=10][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.40it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.40it/s, episode_len=7] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 29.04it/s, episode_len=7][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 29.04it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 29.04it/s, episode_len=10][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 29.04it/s, episode_len=10][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 29.04it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 28.72it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 28.72it/s, episode_len=3] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 28.72it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 28.72it/s, epis
...[truncated]
================================================================================
Trial 70 | time = 2025-11-18T05:03:20.344696
Command: python -m src.train_rl_ppo --config config_trial_070.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.04014
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  3.93
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  7.471
  - reward_weights.physchem: 0.4328  ->  0.3526
  - reward_weights.target_prob: 0.468  ->  0.2835
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3426
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.0317

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.04014, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=1] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=7] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=7][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=13][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.87it/s, episode_len=13][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.87it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.87it/s, episode_len=7] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.87it/s, episode_len=8][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.87it/s, episode_len=4][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.87it/s, episode_len=3][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.87it/s, episode_len=17][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 26.64it/s, episode_len=17][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 26.64it/s, episode_len=8] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 26.64it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 26.64it/s, episode_len=1] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 26.64it/s, episode_len=2][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 26.64it/s, episode_len=14][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 32.55it/s, episode_len=14][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 32.55it/s, episode_len=17][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 32.55it/s, episode_len=16][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 32.55it/s, episode_len=5] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 32.55it/s, episode_len=19][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.63it/s, episode_len=19][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.63it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.63it/s, episode_len=11][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.63it/s, episode_len=12][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.63it/s, episode_len=19][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 30.13it/s, episode_len=19][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 30.13it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 30.13it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 30.13it/s, episode_len=3] [A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 30.13it/s, episode_len=7][A

Collecting trajectories:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/64 [00:01<00:01, 31.35it/s, epis
...[truncated]
================================================================================
Trial 71 | time = 2025-11-18T05:42:23.284616
Command: python -m src.train_rl_ppo --config config_trial_071.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.08889
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  2.852
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  9.132
  - reward_weights.physchem: 0.4328  ->  0.2275
  - reward_weights.target_prob: 0.468  ->  0.3467
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.105
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03207

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.08889, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=4] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=9][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=8][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.37it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.37it/s, episode_len=18][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.37it/s, episode_len=3] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.37it/s, episode_len=6][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.37it/s, episode_len=16][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.37it/s, episode_len=8] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.37it/s, episode_len=7][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 29.37it/s, episode_len=7][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 29.37it/s, episode_len=4][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 29.37it/s, episode_len=2][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 29.37it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 29.37it/s, episode_len=2] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 29.37it/s, episode_len=6][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 29.37it/s, episode_len=7][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 29.37it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 39.45it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 39.45it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 39.45it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 39.45it/s, episode_len=1] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 39.45it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 39.45it/s, episode_len=12][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 38.03it/s, episode_len=12][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 38.03it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 38.03it/s, episode_len=12][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 38.03it/s, episode_len=1] [A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 38.03it/s, episode_len=4][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 38.03it/s, episode_len=9][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 38.03it/s, episode_len=20][A

Collecting trajectories:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 30/64 [00:00<00:00, 40.88it/s, episode_l
...[truncated]
================================================================================
Trial 72 | time = 2025-11-18T06:20:48.799764
Command: python -m src.train_rl_ppo --config config_trial_072.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.08332
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  3.904
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  8.214
  - reward_weights.physchem: 0.4328  ->  0.4831
  - reward_weights.target_prob: 0.468  ->  0.3106
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1437
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03092

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.08332, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=17][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=17][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.21it/s, episode_len=19][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.08it/s, episode_len=19][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.08it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.08it/s, episode_len=14][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.08it/s, episode_len=9] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.08it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.39it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.39it/s, episode_len=10][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.39it/s, episode_len=2] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.39it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.39it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 24.32it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 24.32it/s, episode_len=3] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 24.32it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 24.32it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 24.32it/s, episode_len=13][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 28.07it/s, episode_len=13][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 28.07it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 28.07it/s, episode_len=2] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 28.07it/s, episode_len=8][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 28.07it/s, episode_len=8][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 28.07it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 33.23it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 33.23it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 33.23it/s, episode_len=18][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 33.23it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 33.23it/s, episode_len=13][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 31.67it/s, episode_len=13][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 31.67it/s, episode_len=11][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 31.67it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 31.67it/s, episode_len=2] [A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 31.67it/s, e
...[truncated]
================================================================================
Trial 73 | time = 2025-11-18T06:59:17.603116
Command: python -m src.train_rl_ppo --config config_trial_073.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.01946
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  2.582
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  7.24
  - reward_weights.physchem: 0.4328  ->  0.3892
  - reward_weights.target_prob: 0.468  ->  0.418
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3352
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03864

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.01946, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=1] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=12][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=6] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=1] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.10it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 19.71it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 19.71it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 19.71it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 19.71it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 19.71it/s, episode_len=8] [A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 23.68it/s, episode_len=8][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 23.68it/s, episode_len=6][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 23.68it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 23.68it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 23.68it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 26.37it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 26.37it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 26.37it/s, episode_len=16][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 26.37it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 26.37it/s, episode_len=3] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 29.07it/s, episode_len=3][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 29.07it/s, episode_len=10][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 29.07it/s, episode_len=5] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 29.07it/s, episode_len=1][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 29.07it/s, episode_len=8][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 29.07it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 29.07it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 35.38it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 35.38it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 35.38it/s, episode_len=8] [A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 35.38it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 35.38it/s, episode_len=6] [A

Collecting trajectories:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/64 [00:01<00:00, 36.48it/s, ep
...[truncated]
================================================================================
Trial 74 | time = 2025-11-18T07:38:09.074565
Command: python -m src.train_rl_ppo --config config_trial_074.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.0154
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  3.742
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  9.42
  - reward_weights.physchem: 0.4328  ->  0.4072
  - reward_weights.target_prob: 0.468  ->  0.4846
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1122
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03486

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.0154, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=3][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.67it/s, episode_len=3][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.67it/s, episode_len=3][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.67it/s, episode_len=2][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.67it/s, episode_len=17][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.67it/s, episode_len=13][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 16.10it/s, episode_len=13][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 16.10it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 16.10it/s, episode_len=1] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 16.10it/s, episode_len=4][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 16.10it/s, episode_len=18][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 23.84it/s, episode_len=18][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 23.84it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 23.84it/s, episode_len=6] [A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 23.84it/s, episode_len=3][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 23.84it/s, episode_len=8][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 23.84it/s, episode_len=2][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 23.84it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 31.42it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 31.42it/s, episode_len=5] [A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 31.42it/s, episode_len=12][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 31.42it/s, episode_len=20][A

Collecting trajectories:  23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:00<00:01, 31.42it/s, episode_len=7] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 33.54it/s, episode_len=7][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 33.54it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 33.54it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 33.54it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 33.54it/s, episode_len=4] [A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 30.94it/s, episode_len=4][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 30.94it/s, episode_len=19][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 30.94it/s, episode_len=18][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 30.94it/s, episode_len=4] [A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 30.94it/s, episode_len=5][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:01, 32.69it/s, episode_len=5][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 32.69it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 32.69it/s, episode_len=20]
...[truncated]
================================================================================
Trial 75 | time = 2025-11-18T08:16:29.112077
Command: python -m src.train_rl_ppo --config config_trial_075.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.03155
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  2.73
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  6.453
  - reward_weights.physchem: 0.4328  ->  0.262
  - reward_weights.target_prob: 0.468  ->  0.3127
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1095
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01114

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.03155, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=4][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.69it/s, episode_len=4][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.69it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.69it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.69it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.63it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.63it/s, episode_len=2] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.63it/s, episode_len=18][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.63it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.63it/s, episode_len=8] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.63it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.76it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.76it/s, episode_len=10][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.76it/s, episode_len=2] [A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.76it/s, episode_len=10][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.76it/s, episode_len=15][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.76it/s, episode_len=10][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.76it/s, episode_len=2] [A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 21.76it/s, episode_len=9][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 34.46it/s, episode_len=9][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 34.46it/s, episode_len=12][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 34.46it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 34.46it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 34.46it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 34.46it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.25it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.25it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.25it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.25it/s, episode_len=8] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.25it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 31.34it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 31.34it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 31.34it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 31.34it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 31.34it/s, episode_len=20][A

Collecting trajectories:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/64 [00:01<00:01, 29.73it/s, episode
...[truncated]
================================================================================
Trial 76 | time = 2025-11-18T08:54:49.078510
Command: python -m src.train_rl_ppo --config config_trial_076.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.05755
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  3.111
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  7.767
  - reward_weights.physchem: 0.4328  ->  0.2643
  - reward_weights.target_prob: 0.468  ->  0.3278
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2455
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.02067

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.05755, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=13][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=13][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=4] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.67it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.67it/s, episode_len=13][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.67it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.67it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.26it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.26it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.26it/s, episode_len=13][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.26it/s, episode_len=7] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.26it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 23.54it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 23.54it/s, episode_len=9] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 23.54it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 23.54it/s, episode_len=9] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 23.54it/s, episode_len=16][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 27.78it/s, episode_len=16][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 27.78it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 27.78it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 27.78it/s, episode_len=5] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 27.78it/s, episode_len=4][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 27.78it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.20it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.20it/s, episode_len=15][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.20it/s, episode_len=4] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.20it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 31.20it/s, episode_len=9] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:01<00:01, 31.20it/s, episode_len=14][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 34.27it/s, episode_len=14][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 34.27it/s, episode_len=9] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 34.27it/s, episode_len=17][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 34.27it/s, 
...[truncated]
================================================================================
Trial 77 | time = 2025-11-18T09:34:36.250815
Command: python -m src.train_rl_ppo --config config_trial_077.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.01109
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  3.763
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  9.978
  - reward_weights.physchem: 0.4328  ->  0.4686
  - reward_weights.target_prob: 0.468  ->  0.4182
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1804
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03223

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.01109, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=16][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=1] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=6][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=9][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=18][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.52it/s, episode_len=18][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.52it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.52it/s, episode_len=19][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.52it/s, episode_len=14][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 19.07it/s, episode_len=14][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 19.07it/s, episode_len=7] [A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 19.07it/s, episode_len=3][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 19.07it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 19.07it/s, episode_len=18][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:02, 24.03it/s, episode_len=18][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:02, 24.03it/s, episode_len=9] [A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:02, 24.03it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:02, 24.03it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 24.65it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 24.65it/s, episode_len=17][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 24.65it/s, episode_len=12][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 24.65it/s, episode_len=5] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 24.65it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 26.94it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 26.94it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 26.94it/s, episode_len=3] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 26.94it/s, episode_len=3][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:01<00:01, 26.94it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 29.90it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 29.90it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 29.90it/s, episode_len=8] [A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 29.90it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 29.90it/s, episode_len=16][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 28.73it/s, epis
...[truncated]
================================================================================
Trial 78 | time = 2025-11-18T10:15:56.292804
Command: python -m src.train_rl_ppo --config config_trial_078.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.0717
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  3.651
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  6.008
  - reward_weights.physchem: 0.4328  ->  0.4359
  - reward_weights.target_prob: 0.468  ->  0.3167
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3778
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01122

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.0717, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=7] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=3][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=13][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=14][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.24it/s, episode_len=14][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.24it/s, episode_len=1] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.24it/s, episode_len=6][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.24it/s, episode_len=3][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.24it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.24it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.24it/s, episode_len=7] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 29.40it/s, episode_len=7][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 29.40it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 29.40it/s, episode_len=9] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 29.40it/s, episode_len=7][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 29.40it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:01, 29.40it/s, episode_len=2] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 34.33it/s, episode_len=2][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 34.33it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 34.33it/s, episode_len=17][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 34.33it/s, episode_len=13][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 34.33it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 34.33it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 32.10it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 32.10it/s, episode_len=6] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 32.10it/s, episode_len=11][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 32.10it/s, episode_len=5] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 32.10it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 32.10it/s, episode_len=14][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:01, 35.97it/s, episode_len=14][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:01, 35.97it/s, episode_len=1] [A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:01, 35.97it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 35.97it/s, episo
...[truncated]
================================================================================
Trial 79 | time = 2025-11-18T10:55:22.127895
Command: python -m src.train_rl_ppo --config config_trial_079.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.07046
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  3.723
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  4.482
  - reward_weights.physchem: 0.4328  ->  0.4693
  - reward_weights.target_prob: 0.468  ->  0.4054
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3748
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.007712

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.07046, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=11][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.39it/s, episode_len=11][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.39it/s, episode_len=1] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.39it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.39it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.39it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.61it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.61it/s, episode_len=12][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.61it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.61it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.61it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.18it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.18it/s, episode_len=10][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.18it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.18it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 20.18it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:02, 23.57it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:02, 23.57it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:02, 23.57it/s, episode_len=17][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:02, 23.57it/s, episode_len=8] [A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:02, 23.57it/s, episode_len=15][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 27.07it/s, episode_len=15][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 27.07it/s, episode_len=8] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 27.07it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 27.07it/s, episode_len=6] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 27.07it/s, episode_len=11][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 27.07it/s, episode_len=12][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 32.65it/s, episode_len=12][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 32.65it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 32.65it/s, episode_len=5] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 32.65it/s, episode_len=4][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 32.65it/s, episode_len=15][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 32.65it/s, episode_len=2] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 32.65it/s, episode_len=19][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:00, 37.96it/s, episode_len=19][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:00, 37.96it/s, e
...[truncated]
================================================================================
Trial 80 | time = 2025-11-18T11:35:56.023208
Command: python -m src.train_rl_ppo --config config_trial_080.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.09693
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  2.102
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  9.831
  - reward_weights.physchem: 0.4328  ->  0.2615
  - reward_weights.target_prob: 0.468  ->  0.2777
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1976
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.02054

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.09693, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=9] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=2][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=18][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=9] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=18][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.20it/s, episode_len=18][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.20it/s, episode_len=12][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.20it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.20it/s, episode_len=7] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 17.20it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.07it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.07it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.07it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.07it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.07it/s, episode_len=1] [A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 26.58it/s, episode_len=1][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 26.58it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 26.58it/s, episode_len=13][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 26.58it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 26.58it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 27.11it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 27.11it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 27.11it/s, episode_len=5] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 27.11it/s, episode_len=13][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 27.11it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 29.50it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 29.50it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 29.50it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 29.50it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 29.50it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 28.09it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 28.09it/s, episode_len=16][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 28.09it/s, episode_len=15][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 28.09it/s
...[truncated]
================================================================================
Trial 81 | time = 2025-11-18T12:16:40.351556
Command: python -m src.train_rl_ppo --config config_trial_081.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.06066
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  2.869
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  5.417
  - reward_weights.physchem: 0.4328  ->  0.3693
  - reward_weights.target_prob: 0.468  ->  0.3843
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.06994
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03075

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.06066, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=10][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=10][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.19it/s, episode_len=2] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.56it/s, episode_len=2][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.56it/s, episode_len=5][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.56it/s, episode_len=4][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.56it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.56it/s, episode_len=19][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.50it/s, episode_len=19][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.50it/s, episode_len=4] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.50it/s, episode_len=4][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.50it/s, episode_len=4][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.50it/s, episode_len=2][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.50it/s, episode_len=13][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:02, 19.50it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 30.34it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 30.34it/s, episode_len=9] [A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 30.34it/s, episode_len=2][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 30.34it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 30.34it/s, episode_len=12][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 32.75it/s, episode_len=12][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 32.75it/s, episode_len=9] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 32.75it/s, episode_len=1][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 32.75it/s, episode_len=8][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 32.75it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 32.75it/s, episode_len=7] [A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 37.01it/s, episode_len=7][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 37.01it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 37.01it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 37.01it/s, episode_len=12][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:01<00:01, 37.01it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:01<00:01, 37.01it/s, episode_len=15][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 30.98it/s, episode_len=15][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 30.98it/s, episode_len
...[truncated]
================================================================================
Trial 82 | time = 2025-11-18T12:57:33.393775
Command: python -m src.train_rl_ppo --config config_trial_082.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.05283
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  3.272
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  8.476
  - reward_weights.physchem: 0.4328  ->  0.2336
  - reward_weights.target_prob: 0.468  ->  0.426
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1146
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01932

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.05283, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=4] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=3] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.13it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.01it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.01it/s, episode_len=3] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.01it/s, episode_len=3][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.01it/s, episode_len=2][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.01it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.01it/s, episode_len=2] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.01it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.01it/s, episode_len=5] [A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 30.69it/s, episode_len=5][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 30.69it/s, episode_len=12][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 30.69it/s, episode_len=1] [A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 30.69it/s, episode_len=10][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 30.69it/s, episode_len=7] [A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 30.69it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 36.08it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 36.08it/s, episode_len=15][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 36.08it/s, episode_len=19][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 36.08it/s, episode_len=4] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 36.08it/s, episode_len=11][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 36.08it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 36.12it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 36.12it/s, episode_len=15][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 36.12it/s, episode_len=11][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 36.12it/s, episode_len=1] [A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 36.12it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 36.12it/s, episode_len=2] [A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 36.12it/s, episode_len=20][A

Collecting trajectories:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/64 [00:00<00:00, 38.65it/s, episode_len=20][A

Collecting trajectories:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/64 [00:00<00:00, 38.65it/s, ep
...[truncated]
================================================================================
Trial 83 | time = 2025-11-18T13:37:55.340052
Command: python -m src.train_rl_ppo --config config_trial_083.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.07498
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  2.853
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  3.017
  - reward_weights.physchem: 0.4328  ->  0.4681
  - reward_weights.target_prob: 0.468  ->  0.2657
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3921
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01307

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.07498, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=6][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.64it/s, episode_len=6][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.64it/s, episode_len=1][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.64it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.64it/s, episode_len=9] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.64it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 15.16it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 15.16it/s, episode_len=1] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 15.16it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 15.16it/s, episode_len=2] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 15.16it/s, episode_len=12][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 15.16it/s, episode_len=17][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.94it/s, episode_len=17][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.94it/s, episode_len=10][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.94it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.94it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 25.94it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.15it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.15it/s, episode_len=13][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.15it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.15it/s, episode_len=5] [A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.15it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.68it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.68it/s, episode_len=1] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.68it/s, episode_len=1][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.68it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.68it/s, episode_len=3] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.68it/s, episode_len=6][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.68it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.68it/s, episode_len=7] [A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 39.63it/s, episode_len=7][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 39.63it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 39.63it/s, episode_len=2] [A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 39.63it/s, episode_len=4][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 39.63it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 39.63it/s, epi
...[truncated]
================================================================================
Trial 84 | time = 2025-11-18T14:17:22.446669
Command: python -m src.train_rl_ppo --config config_trial_084.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.03096
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  2.083
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  8.639
  - reward_weights.physchem: 0.4328  ->  0.4305
  - reward_weights.target_prob: 0.468  ->  0.3567
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1166
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01387

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.03096, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=11][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.34it/s, episode_len=11][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.34it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.34it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.34it/s, episode_len=5] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.34it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.16it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.16it/s, episode_len=13][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.16it/s, episode_len=5] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.16it/s, episode_len=6][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.16it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.16it/s, episode_len=6] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.16it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.54it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.54it/s, episode_len=7] [A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.54it/s, episode_len=5][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.54it/s, episode_len=2][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.54it/s, episode_len=9][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.54it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 25.54it/s, episode_len=17][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 33.85it/s, episode_len=17][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 33.85it/s, episode_len=2] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 33.85it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 33.85it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 33.85it/s, episode_len=5] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 33.85it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 35.85it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 35.85it/s, episode_len=6] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 35.85it/s, episode_len=9][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 35.85it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 35.85it/s, episode_len=11][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 35.85it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 37.18it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 37.18it/s, episode_len=5] [A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 37.18it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:00, 37.18it/s, ep
...[truncated]
================================================================================
Trial 85 | time = 2025-11-18T14:58:28.901966
Command: python -m src.train_rl_ppo --config config_trial_085.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.03417
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  3.674
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  3.931
  - reward_weights.physchem: 0.4328  ->  0.3957
  - reward_weights.target_prob: 0.468  ->  0.4845
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.4305
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.04362

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.03417, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=11][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.30it/s, episode_len=11][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.30it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.30it/s, episode_len=1] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.30it/s, episode_len=9][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.30it/s, episode_len=12][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.30it/s, episode_len=2] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.30it/s, episode_len=14][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 20.48it/s, episode_len=14][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 20.48it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 20.48it/s, episode_len=15][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 20.48it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 20.48it/s, episode_len=19][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 23.18it/s, episode_len=19][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 23.18it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 23.18it/s, episode_len=1] [A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 23.18it/s, episode_len=4][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 23.18it/s, episode_len=3][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 23.18it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 23.18it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 30.62it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 30.62it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 30.62it/s, episode_len=5] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 30.62it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 30.62it/s, episode_len=5] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 30.62it/s, episode_len=16][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 33.27it/s, episode_len=16][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 33.27it/s, episode_len=15][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 33.27it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 33.27it/s, episode_len=19][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 33.27it/s, episode_len=17][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:01, 31.85it/s, episode_len=17][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 31.85it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 31.85it/s, episode_len=2] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 31.85it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 31.85it/s, 
...[truncated]
================================================================================
Trial 86 | time = 2025-11-18T15:38:26.064007
Command: python -m src.train_rl_ppo --config config_trial_086.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.03859
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  3.246
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  9.415
  - reward_weights.physchem: 0.4328  ->  0.4122
  - reward_weights.target_prob: 0.468  ->  0.2819
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.4111
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.03315

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.03859, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=6] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=8] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=7][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=7][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=11][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 19.93it/s, episode_len=11][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 19.93it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 19.93it/s, episode_len=15][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 19.93it/s, episode_len=9] [A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:02, 19.93it/s, episode_len=16][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 24.65it/s, episode_len=16][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 24.65it/s, episode_len=4] [A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 24.65it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 24.65it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 24.65it/s, episode_len=6] [A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 24.65it/s, episode_len=14][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.86it/s, episode_len=14][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.86it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.86it/s, episode_len=2] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.86it/s, episode_len=15][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.86it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 31.82it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 31.82it/s, episode_len=11][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 31.82it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 31.82it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 31.82it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 30.94it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 30.94it/s, episode_len=5] [A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 30.94it/s, episode_len=14][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 30.94it/s, episode_len=8] [A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 30.94it/s, episode_len=4][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 30.94it/s, episode_len=3][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 30.94it/s, e
...[truncated]
================================================================================
Trial 87 | time = 2025-11-18T16:20:48.421793
Command: python -m src.train_rl_ppo --config config_trial_087.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.06518
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  3.149
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  3.945
  - reward_weights.physchem: 0.4328  ->  0.2026
  - reward_weights.target_prob: 0.468  ->  0.2595
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1272
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01273

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.06518, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=5][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.67it/s, episode_len=5][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.67it/s, episode_len=7][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.67it/s, episode_len=19][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.67it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.98it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.98it/s, episode_len=7] [A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.98it/s, episode_len=1][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.98it/s, episode_len=5][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.98it/s, episode_len=10][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.98it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.98it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.96it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.96it/s, episode_len=4] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.96it/s, episode_len=2][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.96it/s, episode_len=4][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.96it/s, episode_len=5][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.96it/s, episode_len=8][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.96it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.96it/s, episode_len=2] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.96it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 37.61it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 37.61it/s, episode_len=12][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 37.61it/s, episode_len=6] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 37.61it/s, episode_len=8][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 37.61it/s, episode_len=5][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 37.61it/s, episode_len=1][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 37.61it/s, episode_len=12][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 37.61it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 43.09it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 43.09it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 43.09it/s, episode_len=2] [A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 43.09it/s, episode_len=4][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 43.09it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:00, 43.09it/s, episode_len=20][A

Collecting trajectories:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 30/64 [00:00<00:00, 41.31it/s, episode_l
...[truncated]
================================================================================
Trial 88 | time = 2025-11-18T17:01:49.257833
Command: python -m src.train_rl_ppo --config config_trial_088.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.04772
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  2.694
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  4.03
  - reward_weights.physchem: 0.4328  ->  0.3975
  - reward_weights.target_prob: 0.468  ->  0.2709
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3761
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.0343

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.04772, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.16it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.16it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.16it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.16it/s, episode_len=1] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.16it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.80it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.80it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.80it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.80it/s, episode_len=6] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.80it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 19.86it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 19.86it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 19.86it/s, episode_len=20][A

Collecting trajectories:  14%|â–ˆâ–        | 9/64 [00:00<00:02, 19.86it/s, episode_len=12][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.55it/s, episode_len=12][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.55it/s, episode_len=10][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.55it/s, episode_len=7] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.55it/s, episode_len=16][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.55it/s, episode_len=10][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.55it/s, episode_len=5] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 22.55it/s, episode_len=13][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 31.35it/s, episode_len=13][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 31.35it/s, episode_len=6] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 31.35it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 31.35it/s, episode_len=9] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 31.35it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 33.04it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 33.04it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 33.04it/s, episode_len=19][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 33.04it/s, episode_len=1] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 33.04it/s, episode_len=6][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 33.04it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 35.03it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 35.03it/s, episode_len=13][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 35.03it/s, 
...[truncated]
================================================================================
Trial 89 | time = 2025-11-18T17:41:22.615829
Command: python -m src.train_rl_ppo --config config_trial_089.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.06191
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  2.51
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  5.585
  - reward_weights.physchem: 0.4328  ->  0.4939
  - reward_weights.target_prob: 0.468  ->  0.4841
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2428
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.0271

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.06191, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=3] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=5][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.93it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.93it/s, episode_len=1] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.93it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.93it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.93it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 21.70it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 21.70it/s, episode_len=3] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 21.70it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 21.70it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 21.70it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 25.21it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 25.21it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 25.21it/s, episode_len=12][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 25.21it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 25.21it/s, episode_len=6] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.35it/s, episode_len=6][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.35it/s, episode_len=2][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.35it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.35it/s, episode_len=9] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.35it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.35it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 31.08it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 31.08it/s, episode_len=5] [A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 31.08it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:01<00:01, 31.08it/s, episode_len=8] [A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:01<00:01, 31.08it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 33.34it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 33.34it/s, episode_len=19][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 33.34it/s,
...[truncated]
================================================================================
Trial 90 | time = 2025-11-18T18:22:54.613566
Command: python -m src.train_rl_ppo --config config_trial_090.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.01212
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  3.899
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  8.758
  - reward_weights.physchem: 0.4328  ->  0.4495
  - reward_weights.target_prob: 0.468  ->  0.2632
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3764
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.04966

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.01212, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=8][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.62it/s, episode_len=8][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.62it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.62it/s, episode_len=9] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.62it/s, episode_len=4][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.62it/s, episode_len=6][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:17,  3.62it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 18.02it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 18.02it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 18.02it/s, episode_len=10][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 18.02it/s, episode_len=3] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 18.02it/s, episode_len=1][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 18.02it/s, episode_len=12][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 18.02it/s, episode_len=1] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 18.02it/s, episode_len=18][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 31.41it/s, episode_len=18][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 31.41it/s, episode_len=10][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 31.41it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 31.41it/s, episode_len=15][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 31.41it/s, episode_len=8] [A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 33.49it/s, episode_len=8][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 33.49it/s, episode_len=2][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 33.49it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 33.49it/s, episode_len=13][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 33.49it/s, episode_len=15][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 33.49it/s, episode_len=11][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 36.01it/s, episode_len=11][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 36.01it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 36.01it/s, episode_len=16][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 36.01it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 36.01it/s, episode_len=15][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:01, 33.50it/s, episode_len=15][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:01, 33.50it/s, episode_len=8] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:01, 33.50it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:01, 33.50it/s, episode_len=8] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:01, 33.50it/s, episod
...[truncated]
================================================================================
Trial 91 | time = 2025-11-18T19:04:05.114555
Command: python -m src.train_rl_ppo --config config_trial_091.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.0226
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  2.377
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  4.053
  - reward_weights.physchem: 0.4328  ->  0.4145
  - reward_weights.target_prob: 0.468  ->  0.3545
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.3914
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01769

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.0226, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=18][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=18][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=10][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=1] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.12it/s, episode_len=14][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.23it/s, episode_len=14][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.23it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.23it/s, episode_len=3] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.23it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 16.23it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 21.72it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 21.72it/s, episode_len=7] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 21.72it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 21.72it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 21.72it/s, episode_len=15][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 25.43it/s, episode_len=15][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 25.43it/s, episode_len=12][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 25.43it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 25.43it/s, episode_len=14][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 25.43it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 27.32it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 27.32it/s, episode_len=1] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 27.32it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 27.32it/s, episode_len=4] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 27.32it/s, episode_len=1][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 27.32it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 27.32it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 33.62it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 33.62it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 33.62it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 33.62it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 33.62it/s, episode_len=6] [A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 32.92it/s, episode_len=6][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 32.92it/s,
...[truncated]
================================================================================
Trial 92 | time = 2025-11-18T19:43:23.233132
Command: python -m src.train_rl_ppo --config config_trial_092.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.07121
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  3.063
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  8.22
  - reward_weights.physchem: 0.4328  ->  0.4444
  - reward_weights.target_prob: 0.468  ->  0.356
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2838
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.02035

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.07121, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=14][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=14][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=2] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=3][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=13][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=11][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.09it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 18.43it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 18.43it/s, episode_len=4] [A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 18.43it/s, episode_len=9][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 18.43it/s, episode_len=11][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 18.43it/s, episode_len=3] [A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 18.43it/s, episode_len=1][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 18.43it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 18.43it/s, episode_len=16][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 30.48it/s, episode_len=16][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 30.48it/s, episode_len=9] [A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 30.48it/s, episode_len=18][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 30.48it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 30.48it/s, episode_len=19][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 30.48it/s, episode_len=19][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 30.48it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 30.48it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 30.48it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 30.48it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 28.69it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 28.69it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 28.69it/s, episode_len=5] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 28.69it/s, episode_len=4][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 28.69it/s, episode_len=4][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 28.69it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:01, 33.70it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 33.70it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 33.70it/s, episode_len=1] [A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 33.70it/s, episo
...[truncated]
================================================================================
Trial 93 | time = 2025-11-18T20:23:22.275065
Command: python -m src.train_rl_ppo --config config_trial_093.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.08668
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  3.934
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  7.195
  - reward_weights.physchem: 0.4328  ->  0.4305
  - reward_weights.target_prob: 0.468  ->  0.2599
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.2636
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.02176

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.08668, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=4][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.76it/s, episode_len=4][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.76it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.76it/s, episode_len=4] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.76it/s, episode_len=13][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.76it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.89it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.89it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.89it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:03, 14.89it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.52it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.52it/s, episode_len=11][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.52it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.52it/s, episode_len=12][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.52it/s, episode_len=2] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.52it/s, episode_len=19][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.59it/s, episode_len=19][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.59it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.59it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.59it/s, episode_len=1] [A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.59it/s, episode_len=1][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.59it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 30.50it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 30.50it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 30.50it/s, episode_len=11][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 30.50it/s, episode_len=7] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 30.50it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 31.79it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 31.79it/s, episode_len=15][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 31.79it/s, episode_len=11][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 31.79it/s, episode_len=6] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 31.79it/s, episode_len=17][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 31.79it/s, episode_len=4] [A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:01, 36.20it/s, episode_len=4][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 36.20it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 36.20it/s, epis
...[truncated]
================================================================================
Trial 94 | time = 2025-11-18T21:02:56.208232
Command: python -m src.train_rl_ppo --config config_trial_094.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.04272
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  2.668
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  8.142
  - reward_weights.physchem: 0.4328  ->  0.2554
  - reward_weights.target_prob: 0.468  ->  0.4067
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1123
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01916

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.04272, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=9][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.37it/s, episode_len=9][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.37it/s, episode_len=8][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.37it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.37it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:18,  3.37it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.85it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.85it/s, episode_len=10][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.85it/s, episode_len=5] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.85it/s, episode_len=15][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.85it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 13.85it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.08it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.08it/s, episode_len=3] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.08it/s, episode_len=2][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.08it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.08it/s, episode_len=6] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.08it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.08it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.40it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.40it/s, episode_len=18][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.40it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.40it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.40it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 28.40it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 28.40it/s, episode_len=5] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 28.40it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 28.40it/s, episode_len=13][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 28.40it/s, episode_len=1] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 28.40it/s, episode_len=3][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 28.40it/s, episode_len=6][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 28.40it/s, episode_len=6][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 38.37it/s, episode_len=6][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 38.37it/s, episode_len=10][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 38.37it/s, episode_len=6] [A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:00<00:00, 38.37it/s, epis
...[truncated]
================================================================================
Trial 95 | time = 2025-11-18T21:42:25.462718
Command: python -m src.train_rl_ppo --config config_trial_095.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.05653
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  3.699
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  5.0
  - reward_weights.physchem: 0.4328  ->  0.349
  - reward_weights.target_prob: 0.468  ->  0.4179
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.4317
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.04959

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.05653, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.20it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.20it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.20it/s, episode_len=1] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.20it/s, episode_len=17][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.20it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.25it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.25it/s, episode_len=8] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.25it/s, episode_len=9][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.25it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.25it/s, episode_len=4] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.25it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.78it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.78it/s, episode_len=4] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.78it/s, episode_len=4][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.78it/s, episode_len=2][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.78it/s, episode_len=15][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.78it/s, episode_len=15][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.78it/s, episode_len=2] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.78it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 35.26it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 35.26it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 35.26it/s, episode_len=12][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 35.26it/s, episode_len=20][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 35.26it/s, episode_len=14][A

Collecting trajectories:  27%|â–ˆâ–ˆâ–‹       | 17/64 [00:00<00:01, 35.26it/s, episode_len=17][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 33.70it/s, episode_len=17][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 33.70it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 33.70it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 33.70it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 33.70it/s, episode_len=10][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:01, 32.37it/s, episode_len=10][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:01, 32.37it/s, episode_len=14][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 32.37it/s, episode_len=10][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 32.37it/s, episode_len=4] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:01, 32.37it/s,
...[truncated]
================================================================================
Trial 96 | time = 2025-11-18T22:22:18.303161
Command: python -m src.train_rl_ppo --config config_trial_096.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.06918
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  2.232
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  3.905
  - reward_weights.physchem: 0.4328  ->  0.2192
  - reward_weights.target_prob: 0.468  ->  0.3062
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.09316
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01791

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.06918, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=12][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.27it/s, episode_len=12][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.27it/s, episode_len=12][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.27it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.27it/s, episode_len=8] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.27it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.15it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.15it/s, episode_len=19][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.15it/s, episode_len=13][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.15it/s, episode_len=1] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.15it/s, episode_len=5][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.15it/s, episode_len=14][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.71it/s, episode_len=14][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.71it/s, episode_len=13][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.71it/s, episode_len=4] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.71it/s, episode_len=3][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.71it/s, episode_len=18][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.71it/s, episode_len=3] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 24.71it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 33.09it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 33.09it/s, episode_len=11][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 33.09it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 33.09it/s, episode_len=2] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 33.09it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 34.62it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 34.62it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 34.62it/s, episode_len=4] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 34.62it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 34.62it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 33.68it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 33.68it/s, episode_len=10][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 33.68it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 33.68it/s, episode_len=5] [A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 33.68it/s, episode_len=16][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:00<00:01, 35.32it/s, episode_len=16][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 35.32it/s
...[truncated]
================================================================================
Trial 97 | time = 2025-11-18T23:01:25.860331
Command: python -m src.train_rl_ppo --config config_trial_097.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.02763
  - physchem_constraints.max_repeats: 3  ->  3
  - physchem_constraints.min_entropy: 3.76  ->  3.175
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  5.478
  - reward_weights.physchem: 0.4328  ->  0.4278
  - reward_weights.target_prob: 0.468  ->  0.3201
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.0619
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01545

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.02763, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=1][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.87it/s, episode_len=1][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.87it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.87it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.87it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.88it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.88it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.88it/s, episode_len=20][A

Collecting trajectories:   6%|â–‹         | 4/64 [00:00<00:05, 11.88it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 16.90it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 16.90it/s, episode_len=9] [A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 16.90it/s, episode_len=10][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 16.90it/s, episode_len=20][A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 16.90it/s, episode_len=6] [A

Collecting trajectories:  11%|â–ˆ         | 7/64 [00:00<00:03, 16.90it/s, episode_len=14][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 25.89it/s, episode_len=14][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 25.89it/s, episode_len=4] [A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 25.89it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 25.89it/s, episode_len=20][A

Collecting trajectories:  19%|â–ˆâ–‰        | 12/64 [00:00<00:02, 25.89it/s, episode_len=15][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.16it/s, episode_len=15][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.16it/s, episode_len=3] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.16it/s, episode_len=20][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.16it/s, episode_len=4] [A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.16it/s, episode_len=8][A

Collecting trajectories:  25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:00<00:01, 29.16it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 35.03it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 35.03it/s, episode_len=6] [A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 35.03it/s, episode_len=15][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 35.03it/s, episode_len=20][A

Collecting trajectories:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:01, 35.03it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 35.03it/s, episode_len=20][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 35.03it/s, episode_len=18][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:00<00:01, 35.03it/s, episode_len=4] [A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 35.03it/s, episode_len=12][A

Collecting trajectories:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:01<00:01, 35.03it/s, ep
...[truncated]
================================================================================
Trial 98 | time = 2025-11-18T23:40:51.039898
Command: python -m src.train_rl_ppo --config config_trial_098.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.04293
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  3.199
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  8.454
  - reward_weights.physchem: 0.4328  ->  0.286
  - reward_weights.target_prob: 0.468  ->  0.4705
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.4943
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.007929

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.04293, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.11it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.11it/s, episode_len=2] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.11it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.11it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.11it/s, episode_len=6] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:20,  3.11it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.98it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.98it/s, episode_len=18][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.98it/s, episode_len=11][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.98it/s, episode_len=1] [A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.98it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.98it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 23.25it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 23.25it/s, episode_len=20][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 23.25it/s, episode_len=19][A

Collecting trajectories:  17%|â–ˆâ–‹        | 11/64 [00:00<00:02, 23.25it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:02, 24.11it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:02, 24.11it/s, episode_len=3] [A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:02, 24.11it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:02, 24.11it/s, episode_len=11][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:02, 24.11it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.02it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.02it/s, episode_len=5] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.02it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.02it/s, episode_len=20][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 28.02it/s, episode_len=18][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 29.53it/s, episode_len=18][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 29.53it/s, episode_len=1] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 29.53it/s, episode_len=20][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:00<00:01, 29.53it/s, episode_len=14][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 29.53it/s, episode_len=1] [A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 29.53it/s, episode_len=2][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 29.53it/s, episode_len=7][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 29.53it/s, episode_len=5][A

Collecting trajectories:  34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:01<00:01, 29.53it/s, 
...[truncated]
================================================================================
Trial 99 | time = 2025-11-19T00:20:03.643134
Command: python -m src.train_rl_ppo --config config_trial_099.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.02573
  - physchem_constraints.max_repeats: 3  ->  4
  - physchem_constraints.min_entropy: 3.76  ->  2.254
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  6.267
  - reward_weights.physchem: 0.4328  ->  0.283
  - reward_weights.target_prob: 0.468  ->  0.3762
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.1179
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.005497

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.02573, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=4][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.75it/s, episode_len=4][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.75it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.75it/s, episode_len=2] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.75it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:16,  3.75it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.54it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.54it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.54it/s, episode_len=20][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.54it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.39it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.39it/s, episode_len=11][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.39it/s, episode_len=3] [A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.39it/s, episode_len=9][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.39it/s, episode_len=20][A

Collecting trajectories:  12%|â–ˆâ–Ž        | 8/64 [00:00<00:03, 18.39it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.91it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.91it/s, episode_len=7] [A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.91it/s, episode_len=2][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.91it/s, episode_len=20][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.91it/s, episode_len=7] [A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.91it/s, episode_len=11][A

Collecting trajectories:  20%|â–ˆâ–ˆ        | 13/64 [00:00<00:01, 25.91it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 32.80it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 32.80it/s, episode_len=1] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 32.80it/s, episode_len=10][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 32.80it/s, episode_len=9] [A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 32.80it/s, episode_len=20][A

Collecting trajectories:  30%|â–ˆâ–ˆâ–‰       | 19/64 [00:00<00:01, 32.80it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 35.75it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 35.75it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 35.75it/s, episode_len=6] [A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:00<00:01, 35.75it/s, episode_len=20][A

Collecting trajectories:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:01<00:01, 35.75it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 34.37it/s, episode_len=20][A

Collecting trajectories:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:01<00:01, 34.37it/s, epis
...[truncated]
================================================================================
Trial 100 | time = 2025-11-19T00:58:55.399259
Command: python -m src.train_rl_ppo --config config_trial_100.yaml --target LRP1
Return code: 0
Parsed metric (Best reward): 0.0
Changed parameters:
  - rl.entropy_coef: 0.05078  ->  0.04796
  - physchem_constraints.max_repeats: 3  ->  2
  - physchem_constraints.min_entropy: 3.76  ->  3.113
  - physchem_constraints.entropy_penalty_coef: 7.435  ->  4.79
  - reward_weights.physchem: 0.4328  ->  0.4328
  - reward_weights.target_prob: 0.468  ->  0.3088
  - active_selection.ucb.diversity_weight: 0.4659  ->  0.4086
  - rl_enhanced.kl_penalty.beta: 0.04325  ->  0.01681

STDOUT:


STDERR:
[train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.04796, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=19][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=19][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=14][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=15][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=2] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.18it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.93it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.93it/s, episode_len=12][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.93it/s, episode_len=16][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.93it/s, episode_len=20][A

Collecting trajectories:   9%|â–‰         | 6/64 [00:00<00:03, 15.93it/s, episode_len=7] [A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.39it/s, episode_len=7][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.39it/s, episode_len=6][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.39it/s, episode_len=8][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.39it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 22.39it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.18it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.18it/s, episode_len=2] [A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.18it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.18it/s, episode_len=17][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.18it/s, episode_len=1] [A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.18it/s, episode_len=4][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.18it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 34.38it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 34.38it/s, episode_len=20][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 34.38it/s, episode_len=8] [A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 34.38it/s, episode_len=4][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 34.38it/s, episode_len=6][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 34.38it/s, episode_len=10][A

Collecting trajectories:  31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:00<00:01, 34.38it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 38.30it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 38.30it/s, episode_len=6] [A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:00<00:00, 38.30it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:00, 38.30it/s, episode_len=20][A

Collecting trajectories:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:01<00:00, 38.30it/s, ep
...[truncated]
