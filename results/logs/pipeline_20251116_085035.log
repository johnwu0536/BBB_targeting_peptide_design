2025-11-16 08:50:35,472 - INFO - Starting BBB-penetrating peptide design pipeline
2025-11-16 08:50:35,472 - INFO - Target receptor: LRP1
2025-11-16 08:50:35,472 - INFO - Using device: cuda
2025-11-16 08:50:35,472 - INFO - Step 1: Training classifier
2025-11-16 08:50:35,472 - INFO - Running: python -m src.train_classifier --config config.yaml
2025-11-16 08:50:41,466 - INFO - Command succeeded: python -m src.train_classifier --config config.yaml
2025-11-16 08:50:41,467 - INFO - Output: [train_classifier] Using device: cuda
[train_classifier] Loading data...
[data_loader] Loaded 214 sequences (37 positive, 177 negative)
[data_loader] Created loaders - Train: 5 batches, Val: 1 batches, Test: 1 batches
[train_classifier] Creating model...
[classifier_factory] Model config:
  - type: basic
  - embedding_dim: 128
  - hidden_dim: 256
  - vocab_size: 21
  - dropout: 0.3
  - num_classes: 2
  - type: basic
[train_classifier] Model parameters: 2,436,035
[train_classifier] Starting training...
Epoch   1 | Train Loss: 0.5732 | Val Loss: 0.5055 | Acc: 0.8125 | F1: 0.0000 | AUC: 0.6282 | Val samples: 32 (6+/26-)
[train_classifier] Saved best model to checkpoints/classifier_best.pth
Epoch   2 | Train Loss: 0.4153 | Val Loss: 0.4528 | Acc: 0.8125 | F1: 0.4000 | AUC: 0.7885 | Val samples: 32 (6+/26-)
[train_classifier] Saved best model to checkpoints/classifier_best.pth
Epoch   3 | Train Loss: 0.3389 | Val Loss: 0.4113 | Acc: 0.8125 | F1: 0.4000 | AUC: 0.8077 | Val samples: 32 (6+/26-)
[train_classifier] Saved best model to checkpoints/classifier_best.pth
Epoch   4 | Train Loss: 0.2646 | Val Loss: 0.4796 | Acc: 0.8750 | F1: 0.7143 | AUC: 0.8013 | Val samples: 32 (6+/26-)
Epoch   5 | Train Loss: 0.2520 | Val Loss: 0.5121 | Acc: 0.8438 | F1: 0.5455 | AUC: 0.8141 | Val samples: 32 (6+/26-)
Epoch   6 | Train Loss: 0.1702 | Val Loss: 0.6005 | Acc: 0.7812 | F1: 0.5882 | AUC: 0.7949 | Val samples: 32 (6+/26-)
Epoch   7 | Train Loss: 0.1295 | Val Loss: 0.5451 | Acc: 0.8438 | F1: 0.6154 | AUC: 0.8013 | Val samples: 32 (6+/26-)
Epoch   8 | Train Loss: 0.0972 | Val Loss: 0.9499 | Acc: 0.7188 | F1: 0.5263 | AUC: 0.8141 | Val samples: 32 (6+/26-)
Epoch   9 | Train Loss: 0.0938 | Val Loss: 0.6096 | Acc: 0.8125 | F1: 0.5000 | AUC: 0.7949 | Val samples: 32 (6+/26-)
Epoch  10 | Train Loss: 0.1245 | Val Loss: 0.9147 | Acc: 0.8438 | F1: 0.6667 | AUC: 0.8013 | Val samples: 32 (6+/26-)
Epoch  11 | Train Loss: 0.0808 | Val Loss: 1.0200 | Acc: 0.8438 | F1: 0.6667 | AUC: 0.7949 | Val samples: 32 (6+/26-)
Epoch  12 | Train Loss: 0.0536 | Val Loss: 0.7470 | Acc: 0.8750 | F1: 0.6667 | AUC: 0.7949 | Val samples: 32 (6+/26-)
Epoch  13 | Train Loss: 0.0348 | Val Loss: 0.9273 | Acc: 0.8438 | F1: 0.6154 | AUC: 0.7628 | Val samples: 32 (6+/26-)
[train_classifier] Early stopping at epoch 13

[train_classifier] Final evaluation on test set...
Test Results:
  Loss: 0.0784
  Accuracy: 0.9688
  F1 Score: 0.8571
  AUC: 0.9885
  Confusion Matrix:
    True Negatives: 28
    False Positives: 1
    False Negatives: 0
    True Positives: 3
[train_classifier] Training completed. Metrics saved to results/classifier_metrics.json

2025-11-16 08:50:41,467 - INFO - Step 2: Temperature calibration
2025-11-16 08:50:41,467 - INFO - Running: python -m src.calibrate_temperature --config config.yaml
2025-11-16 08:50:52,029 - INFO - Command succeeded: python -m src.calibrate_temperature --config config.yaml
2025-11-16 08:50:52,029 - INFO - Output: [calibrate_temperature] Using device: cuda
[calibrate_temperature] Loading model from checkpoints/classifier_best.pth
[calibrate_temperature] Loading validation data...
[data_loader] Loaded 214 sequences (37 positive, 177 negative)
[data_loader] Created loaders - Train: 5 batches, Val: 1 batches, Test: 1 batches
[calibrate_temperature] Starting temperature calibration...
[calibrate_temperature] Calibrated temperature: 1.3790
[calibrate_temperature] Evaluating calibration...
[calibrate_temperature] Expected Calibration Error:
  Original: 0.0974
  Calibrated: 0.1397
  Improvement: -0.0422
[calibrate_temperature] Temperature parameter saved to results/temperature.json
[calibrate_temperature] Calibrated model saved to checkpoints/classifier_best_calibrated.pth

2025-11-16 08:50:52,030 - INFO - Step 3: RL training
2025-11-16 08:50:52,030 - INFO - Running: python -m src.train_rl_ppo --config config.yaml --target LRP1
2025-11-16 08:50:58,177 - ERROR - Command failed: python -m src.train_rl_ppo --config config.yaml --target LRP1
2025-11-16 08:50:58,178 - ERROR - Error output: [train_rl_ppo] Using device: cuda
[train_rl_ppo] RL config: lr=1e-05, gamma=0.99, eps=0.2, entropy_coef=0.05078, value_coef=0.5, max_grad_norm=0.5, ppo_epochs=4, episodes/epoch=64, sample_temp=1.2, decode_temp=0.8
[train_rl_ppo] Loading classifier...
[train_rl_ppo] Building basic classifier with params: {'embedding_dim': 128, 'hidden_dim': 256, 'vocab_size': 21, 'dropout': 0.3, 'num_classes': 2, 'type': 'basic'}
[train_rl_ppo] Model moved to device: cuda
[train_rl_ppo] Loaded classifier from checkpoints/classifier_best.pth
[train_rl_ppo] Starting RL training...

PPO Training:   0%|          | 0/1000 [00:00<?, ?epoch/s]

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s][A

Collecting trajectories:   0%|          | 0/64 [00:00<?, ?it/s, episode_len=18][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=18][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=6] [A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=20][A

Collecting trajectories:   2%|â–         | 1/64 [00:00<00:19,  3.17it/s, episode_len=9] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.25it/s, episode_len=9][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.25it/s, episode_len=6][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.25it/s, episode_len=18][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.25it/s, episode_len=2] [A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.25it/s, episode_len=13][A

Collecting trajectories:   8%|â–Š         | 5/64 [00:00<00:04, 14.25it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.67it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.67it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.67it/s, episode_len=20][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.67it/s, episode_len=12][A

Collecting trajectories:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:02, 23.67it/s, episode_len=5] [A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.32it/s, episode_len=5][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.32it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.32it/s, episode_len=20][A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.32it/s, episode_len=2] [A

Collecting trajectories:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:01, 27.32it/s, episode_len=19][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.22it/s, episode_len=19][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.22it/s, episode_len=3] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.22it/s, episode_len=11][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.22it/s, episode_len=6] [A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.22it/s, episode_len=15][A

Collecting trajectories:  28%|â–ˆâ–ˆâ–Š       | 18/64 [00:00<00:01, 29.22it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 34.13it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 34.13it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 34.13it/s, episode_len=20][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:00<00:01, 34.13it/s, episode_len=13][A

Collecting trajectories:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:01<00:01, 34.13it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 31.78it/s, episode_len=20][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 31.78it/s, episode_len=19][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 31.78it/s, episode_len=3] [A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 31.78it/s, episode_len=13][A

Collecting trajectories:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:01<00:01, 31.78it/s, episode_len=20][A

Collecting trajectories:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 31/64 [00:01<00:01, 32.82it/s, episode_len=20][A

Collecting trajectories:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 31/64 [00:01<00:01, 32.82it/s, episode_len=20][A

Collecting trajectories:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 31/64 [00:01<00:01, 32.82it/s, episode_len=20][A

Collecting trajectories:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 31/64 [00:01<00:01, 32.82it/s, episode_len=3] [A

Collecting trajectories:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 31/64 [00:01<00:01, 32.82it/s, episode_len=20][A

Collecting trajectories:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/64 [00:01<00:00, 32.58it/s, episode_len=20][A

Collecting trajectories:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/64 [00:01<00:00, 32.58it/s, episode_len=20][A

Collecting trajectories:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/64 [00:01<00:00, 32.58it/s, episode_len=10][A

Collecting trajectories:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/64 [00:01<00:00, 32.58it/s, episode_len=4] [A

Collecting trajectories:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/64 [00:01<00:00, 32.58it/s, episode_len=6][A

Collecting trajectories:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/64 [00:01<00:00, 32.58it/s, episode_len=20][A

Collecting trajectories:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 40/64 [00:01<00:00, 35.39it/s, episode_len=20][A

Collecting trajectories:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 40/64 [00:01<00:00, 35.39it/s, episode_len=17][A

Collecting trajectories:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 40/64 [00:01<00:00, 35.39it/s, episode_len=20][A

Collecting trajectories:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 40/64 [00:01<00:00, 35.39it/s, episode_len=17][A

Collecting trajectories:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 40/64 [00:01<00:00, 35.39it/s, episode_len=20][A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 44/64 [00:01<00:00, 32.28it/s, episode_len=20][A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 44/64 [00:01<00:00, 32.28it/s, episode_len=7] [A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 44/64 [00:01<00:00, 32.28it/s, episode_len=20][A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 44/64 [00:01<00:00, 32.28it/s, episode_len=20][A

Collecting trajectories:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 44/64 [00:01<00:00, 32.28it/s, episode_len=20][A

Collecting trajectories:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 48/64 [00:01<00:00, 31.09it/s, episode_len=20][A

Collecting trajectories:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 48/64 [00:01<00:00, 31.09it/s, episode_len=1] [A

Collecting trajectories:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 48/64 [00:01<00:00, 31.09it/s, episode_len=20][A

Collecting trajectories:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 48/64 [00:01<00:00, 31.09it/s, episode_len=17][A

Collecting trajectories:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 48/64 [00:01<00:00, 31.09it/s, episode_len=14][A

Collecting trajectories:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 52/64 [00:01<00:00, 32.82it/s, episode_len=14][A

Collecting trajectories:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 52/64 [00:01<00:00, 32.82it/s, episode_len=9] [A

Collecting trajectories:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 52/64 [00:01<00:00, 32.82it/s, episode_len=4][A

Collecting trajectories:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 52/64 [00:01<00:00, 32.82it/s, episode_len=7][A

Collecting trajectories:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 52/64 [00:01<00:00, 32.82it/s, episode_len=18][A

Collecting trajectories:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 52/64 [00:01<00:00, 32.82it/s, episode_len=9] [A

Collecting trajectories:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 57/64 [00:01<00:00, 35.76it/s, episode_len=9][A

Collecting trajectories:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 57/64 [00:01<00:00, 35.76it/s, episode_len=20][A

Collecting trajectories:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 57/64 [00:01<00:00, 35.76it/s, episode_len=4] [A

Collecting trajectories:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 57/64 [00:01<00:00, 35.76it/s, episode_len=20][A

Collecting trajectories:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 57/64 [00:02<00:00, 35.76it/s, episode_len=20][A

Collecting trajectories:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 61/64 [00:02<00:00, 34.31it/s, episode_len=20][A

Collecting trajectories:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 61/64 [00:02<00:00, 34.31it/s, episode_len=18][A

Collecting trajectories:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 61/64 [00:02<00:00, 34.31it/s, episode_len=15][A

Collecting trajectories:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 61/64 [00:02<00:00, 34.31it/s, episode_len=7] [A

                                                                                       [A
PPO Training:   0%|          | 0/1000 [00:02<?, ?epoch/s, avg_reward=-0.0535, loss=0.0138, best=-inf]
PPO Training:   0%|          | 0/1000 [00:02<?, ?epoch/s, avg_reward=-0.0535, loss=0.0138, best=-inf]
Traceback (most recent call last):
  File "/home/wuhaigang/anaconda3/envs/openmm-env/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wuhaigang/anaconda3/envs/openmm-env/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wuhaigang/WHG/NiuNiu1/src/train_rl_ppo.py", line 678, in <module>
    main()
  File "/home/wuhaigang/WHG/NiuNiu1/src/train_rl_ppo.py", line 579, in main
    f"Epoch {epoch}: Avg Reward = {avg_reward:.4f}, "
ValueError: Invalid format specifier

2025-11-16 08:50:58,178 - ERROR - RL training failed
